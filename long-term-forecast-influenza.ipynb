{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Term Forecast Influenza using Web data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US, AU, KR influenza load\n",
    "\n",
    "flu = {\n",
    "    'us': pd.read_csv('./data/US_flu.csv'), \n",
    "    'au': pd.read_csv('./data/AU_flu.csv'),\n",
    "    'kr': pd.read_csv('./data/KR_flu.csv') \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weeks</th>\n",
       "      <th>ILI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-01</td>\n",
       "      <td>19.8284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10-02</td>\n",
       "      <td>18.2749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10-03</td>\n",
       "      <td>19.2606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10-04</td>\n",
       "      <td>19.2495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-05</td>\n",
       "      <td>20.8877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>20-16</td>\n",
       "      <td>25.7453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>20-17</td>\n",
       "      <td>20.2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>20-18</td>\n",
       "      <td>16.2086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>20-19</td>\n",
       "      <td>13.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>20-20</td>\n",
       "      <td>12.3794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     weeks      ILI\n",
       "0    10-01  19.8284\n",
       "1    10-02  18.2749\n",
       "2    10-03  19.2606\n",
       "3    10-04  19.2495\n",
       "4    10-05  20.8877\n",
       "..     ...      ...\n",
       "535  20-16  25.7453\n",
       "536  20-17  20.2111\n",
       "537  20-18  16.2086\n",
       "538  20-19  13.5240\n",
       "539  20-20  12.3794\n",
       "\n",
       "[540 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flu['us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us flu data shape: (540, 2)\n",
      "au flu data shape: (540, 2)\n",
      "kr flu data shape: (540, 2)\n"
     ]
    }
   ],
   "source": [
    "for nation in flu:\n",
    "    print(f'{nation} flu data shape: {flu[nation].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US, AU, KR Google trends\n",
    "\n",
    "trends = {\n",
    "    'us': pd.read_csv('./data/US_trends.csv'), \n",
    "    'au': pd.read_csv('./data/AU_trends.csv'), \n",
    "    'kr': pd.read_csv('./data/KR_trends.csv') \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weeks</th>\n",
       "      <th>influenza</th>\n",
       "      <th>H1N1 flu</th>\n",
       "      <th>swine flu</th>\n",
       "      <th>Flu</th>\n",
       "      <th>H1N1</th>\n",
       "      <th>seasonal flu</th>\n",
       "      <th>H1N1 virus</th>\n",
       "      <th>flu virus</th>\n",
       "      <th>H1N1 swine flu</th>\n",
       "      <th>...</th>\n",
       "      <th>polio epidemic</th>\n",
       "      <th>Afluria seasonal flu</th>\n",
       "      <th>hemolytic uremic syndrome HUS</th>\n",
       "      <th>croup</th>\n",
       "      <th>Jeff Duchin</th>\n",
       "      <th>inactivated vaccine</th>\n",
       "      <th>fevers aches</th>\n",
       "      <th>invasive pneumococcal disease</th>\n",
       "      <th>Sudden Acute Respiratory Syndrome</th>\n",
       "      <th>commonly infect humans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17-35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17-36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17-37</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17-38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17-39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>20-16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>20-17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>20-18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>20-19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>20-20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     weeks  influenza  H1N1 flu  swine flu  Flu  H1N1  seasonal flu  \\\n",
       "0    17-35          0         0          0    6     0             0   \n",
       "1    17-36          1         0          0    8     0             0   \n",
       "2    17-37          1         0          0    9     0             0   \n",
       "3    17-38          1         0          0   11     0             0   \n",
       "4    17-39          1         0          0   12     0             0   \n",
       "..     ...        ...       ...        ...  ...   ...           ...   \n",
       "137  20-16          2         0          1   24     2             3   \n",
       "138  20-17          2         0          1   19     1             2   \n",
       "139  20-18          2         0          1   18     1             2   \n",
       "140  20-19          2         0          1   16     1             2   \n",
       "141  20-20          2         0          1   15     1             2   \n",
       "\n",
       "     H1N1 virus  flu virus  H1N1 swine flu  ...  polio epidemic  \\\n",
       "0             0          1               0  ...               1   \n",
       "1             0          2               0  ...               0   \n",
       "2             0          2               0  ...               0   \n",
       "3             0          2               0  ...               0   \n",
       "4             0          2               0  ...               0   \n",
       "..          ...        ...             ...  ...             ...   \n",
       "137           1         10               1  ...               5   \n",
       "138           1         10               1  ...               8   \n",
       "139           1          8               1  ...               6   \n",
       "140           1          6               1  ...               7   \n",
       "141           1          5               1  ...               5   \n",
       "\n",
       "     Afluria seasonal flu  hemolytic uremic syndrome HUS  croup  Jeff Duchin  \\\n",
       "0                       0                              0     40            0   \n",
       "1                       0                              0     50            0   \n",
       "2                       0                              0     51            0   \n",
       "3                       0                              0     58            0   \n",
       "4                       0                              0     68            0   \n",
       "..                    ...                            ...    ...          ...   \n",
       "137                     0                              0      8            1   \n",
       "138                     0                              0     10            0   \n",
       "139                     0                              0      5            0   \n",
       "140                     0                              0      7            0   \n",
       "141                     0                              0      8            0   \n",
       "\n",
       "     inactivated vaccine  fevers aches  invasive pneumococcal disease  \\\n",
       "0                     46             0                              0   \n",
       "1                     50            14                             14   \n",
       "2                     46            14                             14   \n",
       "3                     87             0                              0   \n",
       "4                     54            14                              0   \n",
       "..                   ...           ...                            ...   \n",
       "137                   76             0                              0   \n",
       "138                   41             0                              0   \n",
       "139                   56             0                             11   \n",
       "140                   28             0                              0   \n",
       "141                   36            11                              0   \n",
       "\n",
       "     Sudden Acute Respiratory Syndrome  commonly infect humans  \n",
       "0                                    0                       0  \n",
       "1                                    0                       0  \n",
       "2                                    0                       0  \n",
       "3                                    0                       0  \n",
       "4                                    0                       0  \n",
       "..                                 ...                     ...  \n",
       "137                                  0                       0  \n",
       "138                                  0                       0  \n",
       "139                                  0                       0  \n",
       "140                                  0                       0  \n",
       "141                                  0                       0  \n",
       "\n",
       "[142 rows x 1001 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends['us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us trends data shape: (142, 1001)\n",
      "au trends data shape: (142, 966)\n",
      "kr trends data shape: (142, 1001)\n"
     ]
    }
   ],
   "source": [
    "for nation in trends:\n",
    "    print(f'{nation} trends data shape: {trends[nation].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== us ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 736.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== au ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 965/965 [00:01<00:00, 757.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== kr ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 762.72it/s]\n"
     ]
    }
   ],
   "source": [
    "MIN_PRECIDENCE = 3 # 최소 시간 선행\n",
    "MAX_PRECIDENCE = 15 # 최대 시간 선행\n",
    "precidences = [i for i in range(MIN_PRECIDENCE, MAX_PRECIDENCE+1)]\n",
    "\n",
    "corrs = {}\n",
    "\n",
    "for nation in ['us', 'au', 'kr']:\n",
    "    print('='*10, nation, '='*10)\n",
    "    flu_data = flu[nation]\n",
    "    trends_data = trends[nation]\n",
    "\n",
    "    corrs_by_prcds = []\n",
    "    keywords = trends_data.columns[1:] # weeks를 제외한 모든 열\n",
    "    for keyword in tqdm(keywords):\n",
    "        # correlation을 계산할 데이터 선택\n",
    "        # trends의 weeks와 일치하는 flu데이터\n",
    "        base = flu_data[flu_data.weeks.isin(trends_data.weeks)].ILI.reset_index(drop=True)\n",
    "        target = trends_data[keyword] # 각 키워드의 trends 데이터\n",
    "        \n",
    "        corr_list = []\n",
    "        for prcd in precidences:\n",
    "            corr = base.corr(target.shift(prcd)) # precidence에 따라 trends를 shift/corr 계산\n",
    "            corr_list.append(corr)\n",
    "        corrs_by_prcds.append(corr_list)\n",
    "\n",
    "    corrs_by_prcds = pd.DataFrame(corrs_by_prcds).T # 데이터 프레임 형식\n",
    "    corrs_by_prcds.columns = keywords \n",
    "    corrs_by_prcds.insert(0, 'prcd', precidences) # 첫 번째 열에 precidence를 표시\n",
    "    corrs[nation] = corrs_by_prcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['18-03', '18-04', '18-05', '18-06', '18-07', '18-08', '18-09',\n",
       "       '18-10', '18-11', '18-12', '18-13', '18-14', '18-15', '18-16',\n",
       "       '18-17', '18-18', '18-19', '18-20', '18-21', '18-22', '18-23',\n",
       "       '18-24', '18-25', '18-26', '18-27', '18-28', '18-29', '18-30',\n",
       "       '18-31', '18-32', '18-33', '18-34', '18-35', '18-36', '18-37',\n",
       "       '18-38', '18-39', '18-40', '18-41', '18-42', '18-43', '18-44',\n",
       "       '18-45', '18-46', '18-47', '18-48', '18-49', '18-50', '18-51',\n",
       "       '18-52', '18-53', '19-01', '19-02', '19-03', '19-04', '19-05',\n",
       "       '19-06', '19-07', '19-08', '19-09', '19-10', '19-11', '19-12',\n",
       "       '19-13', '19-14', '19-15', '19-16', '19-17', '19-18', '19-19',\n",
       "       '19-20', '19-21', '19-22', '19-23', '19-24', '19-25', '19-26',\n",
       "       '19-27', '19-28', '19-29', '19-30', '19-31', '19-32', '19-33',\n",
       "       '19-34', '19-35', '19-36', '19-37', '19-38', '19-39', '19-40',\n",
       "       '19-41', '19-42', '19-43', '19-44', '19-45', '19-46', '19-47',\n",
       "       '19-48', '19-49', '19-50', '19-51', '19-52', '20-01', '20-02',\n",
       "       '20-03', '20-04', '20-05', '20-06', '20-07', '20-08', '20-09',\n",
       "       '20-10', '20-11'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends['us'].weeks[20:-10+1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weeks</th>\n",
       "      <th>ILI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>17-50</td>\n",
       "      <td>32.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>17-51</td>\n",
       "      <td>44.3807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>17-52</td>\n",
       "      <td>54.6361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>18-01</td>\n",
       "      <td>53.5765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>18-02</td>\n",
       "      <td>55.2437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     weeks      ILI\n",
       "414  17-50  32.0050\n",
       "415  17-51  44.3807\n",
       "416  17-52  54.6361\n",
       "417  18-01  53.5765\n",
       "418  18-02  55.2437"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = flu['us'][flu['us'].weeks=='18-03'].index[0]\n",
    "\n",
    "flu['us'][idx-5:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flu fever cough', 0.8748608947690277, 3),\n",
       " ('bronchitis', 0.8734438553088462, 3),\n",
       " ('colds', 0.8734000309965851, 6),\n",
       " ('Flu Pneumonia', 0.865196060775432, 3)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(keyword, coef, corrs['us'][keyword].argmax()+MIN_PRECIDENCE) for keyword, coef in corrs['us'].max().sort_values(ascending=False)[1:5].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(flu_data, trends_data, corrs_data, \n",
    "               lookback=5, lookahead=5, topn=1, historic=7):\n",
    "    flu_inputs = []\n",
    "    flu_labels = []\n",
    "    web_past_inputs = []\n",
    "    web_future_inputs = []\n",
    "    week_ranges = []\n",
    "    \n",
    "    keywords_rank = corrs_data.max().sort_values(ascending=False)\n",
    "    keywords_rank = [(keyword, coef, corrs_data[keyword].argmax()+MIN_PRECIDENCE)\n",
    "                     for keyword, coef in keywords_rank[1:topn+1].items()]\n",
    "    \n",
    "    available_weeks = trends_data['weeks'][MAX_PRECIDENCE+lookback:-lookahead+1].values\n",
    "    for year_week in available_weeks:\n",
    "        year, week = map(int, year_week.split('-'))\n",
    "        \n",
    "        flu_pos = flu_data[flu_data.weeks==f'{year}-{week:02}'].index[0]\n",
    "        curr_flu = flu_data[flu_pos-lookback:flu_pos].ILI.values.reshape(-1, 1)\n",
    "        curr_lbl = flu_data[flu_pos:flu_pos+lookahead].ILI.values\n",
    "        \n",
    "        for h in range(1, historic+1):\n",
    "            flu_pos = flu_data[flu_data.weeks==f'{year-h}-{week:02}'].index[0]\n",
    "            historic_flu = flu_data[flu_pos-lookback:flu_pos].ILI.values.reshape(-1, 1)\n",
    "            curr_flu = np.hstack([curr_flu, historic_flu])\n",
    "            \n",
    "        web_past = None\n",
    "        web_future = None\n",
    "        for keyword, coef, prcd in keywords_rank:\n",
    "            web_pos = trends_data[trends_data.weeks==f'{year}-{week:02}'].index[0]\n",
    "            curr_web_past = trends_data[keyword][web_pos-lookback-prcd:web_pos-prcd].values.reshape(-1, 1)\n",
    "            curr_web_future = np.random.uniform(1e-6, 1e-4, (lookahead, 1))\n",
    "            curr_web_future[:prcd] = trends_data[keyword][web_pos-prcd:web_pos-prcd+lookahead].values.reshape(-1, 1)[:prcd]\n",
    "            \n",
    "            if web_past is None:\n",
    "                web_past = curr_web_past\n",
    "            else:\n",
    "                web_past = np.hstack([web_past, curr_web_past])\n",
    "            if web_future is None:\n",
    "                web_future = curr_web_future\n",
    "            else:\n",
    "                web_future = np.hstack([web_future, curr_web_future])\n",
    "            \n",
    "        \n",
    "        flu_inputs.append(curr_flu)\n",
    "        flu_labels.append(curr_lbl)\n",
    "        web_past_inputs.append(web_past)\n",
    "        web_future_inputs.append(web_future)\n",
    "    \n",
    "    return flu_inputs, flu_labels, web_past_inputs, web_future_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weeks</th>\n",
       "      <th>flu fever cough</th>\n",
       "      <th>bronchitis</th>\n",
       "      <th>colds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>17-45</td>\n",
       "      <td>21</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17-46</td>\n",
       "      <td>18</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17-47</td>\n",
       "      <td>14</td>\n",
       "      <td>63</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>17-48</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17-49</td>\n",
       "      <td>27</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17-50</td>\n",
       "      <td>31</td>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17-51</td>\n",
       "      <td>46</td>\n",
       "      <td>92</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17-52</td>\n",
       "      <td>33</td>\n",
       "      <td>98</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18-01</td>\n",
       "      <td>58</td>\n",
       "      <td>90</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18-02</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>18-03</td>\n",
       "      <td>100</td>\n",
       "      <td>78</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>18-04</td>\n",
       "      <td>89</td>\n",
       "      <td>76</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18-05</td>\n",
       "      <td>92</td>\n",
       "      <td>76</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18-06</td>\n",
       "      <td>70</td>\n",
       "      <td>75</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>18-07</td>\n",
       "      <td>58</td>\n",
       "      <td>66</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18-08</td>\n",
       "      <td>41</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>18-09</td>\n",
       "      <td>38</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18-10</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18-11</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18-12</td>\n",
       "      <td>22</td>\n",
       "      <td>53</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weeks  flu fever cough  bronchitis  colds\n",
       "10  17-45               21          55     12\n",
       "11  17-46               18          52     14\n",
       "12  17-47               14          63     13\n",
       "13  17-48               13          61     14\n",
       "14  17-49               27          63     15\n",
       "15  17-50               31          75     18\n",
       "16  17-51               46          92     24\n",
       "17  17-52               33          98     24\n",
       "18  18-01               58          90     18\n",
       "19  18-02               74          78     16\n",
       "20  18-03              100          78     15\n",
       "21  18-04               89          76     15\n",
       "22  18-05               92          76     14\n",
       "23  18-06               70          75     12\n",
       "24  18-07               58          66     12\n",
       "25  18-08               41          59      9\n",
       "26  18-09               38          56     10\n",
       "27  18-10               24          55      9\n",
       "28  18-11               24          54     10\n",
       "29  18-12               22          53      8"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends['us'][['weeks', 'flu fever cough', 'bronchitis', 'colds']][10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 63, 12, 16,  3,  1, 79,  7, 15, 40],\n",
       "       [13, 61, 12, 23,  3,  1, 79,  8, 20, 43],\n",
       "       [27, 63, 14, 24,  5,  1, 84,  6, 26, 41],\n",
       "       [31, 75, 13, 35,  7,  3, 85, 15, 26, 46],\n",
       "       [46, 92, 14, 45, 11,  4, 93, 10, 28, 55]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_data(flu['us'], trends['us'], corrs['us'], topn=10)[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.30000000e+01, 9.80000000e+01, 1.50000000e+01, 5.80000000e+01,\n",
       "        1.10000000e+01, 4.00000000e+00, 1.00000000e+02, 1.70000000e+01,\n",
       "        3.60000000e+01, 6.50000000e+01],\n",
       "       [5.80000000e+01, 9.00000000e+01, 1.80000000e+01, 7.00000000e+01,\n",
       "        1.50000000e+01, 6.00000000e+00, 9.00000000e+01, 1.80000000e+01,\n",
       "        4.50000000e+01, 5.80000000e+01],\n",
       "       [7.40000000e+01, 7.80000000e+01, 2.40000000e+01, 9.70000000e+01,\n",
       "        1.80000000e+01, 5.00000000e+00, 8.60000000e+01, 1.70000000e+01,\n",
       "        5.30000000e+01, 5.70000000e+01],\n",
       "       [7.81622667e-06, 3.49647535e-06, 2.40000000e+01, 7.34253195e-05,\n",
       "        7.35682964e-05, 7.72619588e-06, 7.73248802e-05, 9.18875564e-05,\n",
       "        4.90000000e+01, 8.53496564e-05],\n",
       "       [8.91204701e-05, 4.37305160e-05, 1.80000000e+01, 2.41379610e-05,\n",
       "        7.00203722e-05, 4.97435409e-05, 3.95683328e-05, 8.05447478e-05,\n",
       "        4.40000000e+01, 1.95526973e-05],\n",
       "       [1.46297499e-05, 1.91828053e-05, 1.60000000e+01, 1.29969617e-05,\n",
       "        5.88442420e-05, 8.59256602e-05, 3.75323407e-05, 9.41299302e-05,\n",
       "        4.40000000e+01, 4.11639732e-05],\n",
       "       [7.47764074e-05, 4.30696499e-05, 9.44083107e-05, 3.49991978e-05,\n",
       "        3.35201788e-05, 9.48484158e-05, 9.73898554e-06, 6.33561647e-05,\n",
       "        6.74270111e-06, 9.27289284e-05]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_data(flu['us'], trends['us'], corrs['us'], topn=10, lookahead=7)[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32.005 , 44.3807, 54.6361, 53.5765, 55.2437],\n",
       "       [22.7731, 29.1465, 35.4919, 30.8102, 30.1875],\n",
       "       [19.6571, 24.2021, 24.1083, 19.8859, 19.8985],\n",
       "       [34.178 , 44.6555, 61.1497, 40.7898, 39.3762],\n",
       "       [24.1674, 30.3071, 43.8724, 42.112 , 34.6492],\n",
       "       [36.2552, 45.532 , 60.2598, 49.6368, 46.9072],\n",
       "       [16.6429, 18.5274, 21.2413, 17.6709, 15.434 ],\n",
       "       [25.5659, 33.8185, 34.3172, 26.9151, 29.1063]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_data(flu['us'], trends['us'], None)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag load\n",
    "\n",
    "US_trends_lag = pd.read_csv('./data/v2/US_trends_lag.csv', index_col=0)\n",
    "AU_trends_lag = pd.read_csv('./data/v2/AU_trends_lag.csv', index_col=0)\n",
    "\n",
    "KR_news_lag = pd.read_csv('./data/v2/KR_news_lag.csv', index_col=0)\n",
    "KR_sns_lag = pd.read_csv('./data/v2/KR_sns_lag.csv', index_col=0)\n",
    "KR_trends_lag = pd.read_csv('./data/v2/KR_trends_lag.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max lag load\n",
    "\n",
    "US_trends_max_lag = pd.read_csv('./data/v2/US_trends_max_lag.csv', index_col=0)\n",
    "AU_trends_max_lag = pd.read_csv('./data/v2/AU_trends_max_lag.csv', index_col=0)\n",
    "\n",
    "KR_news_max_lag = pd.read_csv('./data/v2/KR_news_max_lag.csv', index_col=0)\n",
    "KR_sns_max_lag = pd.read_csv('./data/v2/KR_sns_max_lag.csv', index_col=0)\n",
    "KR_trends_max_lag = pd.read_csv('./data/v2/KR_trends_max_lag.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance load\n",
    "\n",
    "US_perform = pd.read_csv('./data/v2/US_perform4.csv', index_col=0)\n",
    "AU_perform = pd.read_csv('./data/v2/AU_perform4.csv', index_col=0)\n",
    "KR_perform = pd.read_csv('./data/v2/KR_perform4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction load\n",
    "\n",
    "with open('./data/v2/US_predict4.pkl', 'rb') as f:\n",
    "    US_predict = pickle.load(f)\n",
    "with open('./data/v2/AU_predict4.pkl', 'rb') as f:\n",
    "    AU_predict = pickle.load(f)\n",
    "with open('./data/v2/KR_predict4.pkl', 'rb') as f:\n",
    "    KR_predict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform_by_num_words\n",
    "\n",
    "with open('./data/v2/perform_by_num_words.pkl', 'rb') as f:\n",
    "    perform_by_num_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Attention, Dense, Concatenate, Average, TimeDistributed, LeakyReLU, Add, Bidirectional, Average\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Encoder_v2(object):\n",
    "    def __init__(self, mode='A', is_sg=False, **kwargs):\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        self.past_year_num = kwargs['past_year_num'] if 'past_year_num' in kwargs else 1\n",
    "        self.news_word_num = kwargs['news_word_num'] if 'news_word_num' in kwargs else 1\n",
    "        self.sns_word_num = kwargs['sns_word_num'] if 'sns_word_num' in kwargs else 1\n",
    "        self.trend_word_num = kwargs['trend_word_num'] if 'trend_word_num' in kwargs else 1\n",
    "        self.layer_size = kwargs['layer_size'] if 'layer_size' in kwargs else 128\n",
    "        self.is_sg = is_sg # single encoder\n",
    "        self.mode = mode # N: news , S: SNS , T: Trend , A: N+S+T\n",
    "        \n",
    "        \n",
    "        ENC_LSTM_UNIT = self.layer_size\n",
    "        DENSE_UNIT = self.layer_size\n",
    "        \n",
    "        print(ENC_LSTM_UNIT, DENSE_UNIT)\n",
    "        \n",
    "        # Encoder1\n",
    "        enc1_inputs = Input(shape=(self.look_back, self.past_year_num+1), name='enc1_inputs')\n",
    "        \n",
    "        enc1_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc1_lstm1')\n",
    "        enc1_outputs1, enc1_h1, enc1_c1 = enc1_lstm1(enc1_inputs)\n",
    "        \n",
    "        # Encoder2\n",
    "        # news\n",
    "        enc2_news_inputs = Input(shape=(self.look_back, self.news_word_num), name='enc2_news_inputs')\n",
    "        \n",
    "        enc2_news_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc2_news_lstm1')\n",
    "        enc2_news_outputs1, enc2_news_h1, enc2_news_c1 = enc2_news_lstm1(enc2_news_inputs)\n",
    "        \n",
    "        enc2_news_outputs = Concatenate(axis=-1, name='enc2_news_outputs_concat')([enc1_outputs1, enc2_news_outputs1])\n",
    "        \n",
    "        enc2_news_states = [\n",
    "            [Concatenate(name='enc2_news_h1')([enc1_h1, enc2_news_h1]), Concatenate(name='enc2_news_c1')([enc1_c1, enc2_news_c1])],\n",
    "        ]\n",
    "        \n",
    "        # sns\n",
    "        enc2_sns_inputs = Input(shape=(self.look_back, self.sns_word_num), name='enc2_sns_inputs')\n",
    "        \n",
    "        enc2_sns_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc2_sns_lstm1')\n",
    "        enc2_sns_outputs1, enc2_sns_h1, enc2_sns_c1 = enc2_sns_lstm1(enc2_sns_inputs)\n",
    "        \n",
    "        enc2_sns_outputs = Concatenate(axis=-1, name='enc2_sns_outputs_concat')([enc1_outputs1, enc2_sns_outputs1])\n",
    "        \n",
    "        enc2_sns_states = [\n",
    "            [Concatenate(name='enc2_sns_h1')([enc1_h1, enc2_sns_h1]), Concatenate(name='enc2_sns_c1')([enc1_c1, enc2_sns_c1])],\n",
    "        ]\n",
    "        \n",
    "        # trend\n",
    "        enc2_trend_inputs = Input(shape=(self.look_back, self.trend_word_num), name='enc2_trend_inputs')\n",
    "        \n",
    "        enc2_trend_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc2_trend_lstm1')\n",
    "        enc2_trend_outputs1, enc2_trend_h1, enc2_trend_c1 = enc2_trend_lstm1(enc2_trend_inputs)\n",
    "        \n",
    "        enc2_trend_outputs = Concatenate(axis=-1, name='enc2_trend_outputs_concat')([enc1_outputs1, enc2_trend_outputs1])\n",
    "        \n",
    "        enc2_trend_states = [\n",
    "            [Concatenate(name='enc2_trend_h1')([enc1_h1, enc2_trend_h1]), Concatenate(name='enc2_trend_c1')([enc1_c1, enc2_trend_c1])],\n",
    "        ]\n",
    "        \n",
    "        # Decoder\n",
    "        if is_sg:\n",
    "            enc2_news_states = [enc2_news_h1, enc2_news_c1]\n",
    "            enc2_news_outputs = enc2_news_outputs1\n",
    "            enc2_sns_states = [enc2_sns_h1, enc2_sns_c1]\n",
    "            enc2_sns_outputs = enc2_sns_outputs1\n",
    "            enc2_trend_states = [enc2_trend_h1, enc2_trend_c1]\n",
    "            enc2_trend_outputs = enc2_trend_outputs1\n",
    "            dec_lstm_unit = ENC_LSTM_UNIT\n",
    "            \n",
    "        else:\n",
    "            enc2_news_states = enc2_news_states[0]\n",
    "            enc2_sns_states = enc2_sns_states[0]\n",
    "            enc2_trend_states = enc2_trend_states[0]\n",
    "            dec_lstm_unit = ENC_LSTM_UNIT * 2\n",
    "        \n",
    "        \n",
    "        # news\n",
    "        dec_news_fn = self.news_word_num if is_sg else self.news_word_num+1\n",
    "        dec_news_inputs = Input(shape=(None, dec_news_fn), name='dec_news_inputs')\n",
    "        \n",
    "        dec_news_lstm1 = LSTM(dec_lstm_unit, return_state=True, return_sequences=True, name='dec_news_lstm1')\n",
    "        dec_news_outputs1, _, _ = dec_news_lstm1(dec_news_inputs, initial_state=enc2_news_states)\n",
    "        \n",
    "        dec_news_attn = Attention(name='dec_news_attn')([dec_news_outputs1, enc2_news_outputs])\n",
    "        \n",
    "        # sns\n",
    "        dec_sns_fn = self.sns_word_num if is_sg else self.sns_word_num+1\n",
    "        dec_sns_inputs = Input(shape=(None, dec_sns_fn), name='dec_sns_inputs')\n",
    "        \n",
    "        dec_sns_lstm1 = LSTM(dec_lstm_unit, return_state=True, return_sequences=True, name='dec_sns_lstm1')\n",
    "        dec_sns_outputs1, _, _ = dec_sns_lstm1(dec_sns_inputs, initial_state=enc2_sns_states)\n",
    "        \n",
    "        dec_sns_attn = Attention(name='dec_sns_attn')([dec_sns_outputs1, enc2_sns_outputs])\n",
    "        \n",
    "        # trend\n",
    "        dec_trend_fn = self.trend_word_num if is_sg else self.trend_word_num+1\n",
    "        dec_trend_inputs = Input(shape=(None, dec_trend_fn), name='dec_trend_inputs')\n",
    "        \n",
    "        dec_trend_lstm1 = LSTM(dec_lstm_unit, return_state=True, return_sequences=True, name='dec_trend_lstm1')\n",
    "        dec_trend_outputs1, _, _ = dec_trend_lstm1(dec_trend_inputs, initial_state=enc2_trend_states)\n",
    "        \n",
    "        dec_trend_attn = Attention(name='dec_trend_attn')([dec_trend_outputs1, enc2_trend_outputs])\n",
    "        \n",
    "        # concat & dense\n",
    "        if mode == 'N':\n",
    "            enc2_inputs = [enc2_news_inputs]\n",
    "            enc2_outputs = enc2_news_outputs\n",
    "            enc2_states = enc2_news_states\n",
    "            dec_inputs = [dec_news_inputs]\n",
    "            dec_attn_concat = dec_news_attn\n",
    "            dec_lstm1 = dec_news_lstm1\n",
    "        elif mode == 'S':\n",
    "            enc2_inputs = [enc2_sns_inputs]\n",
    "            enc2_outputs = enc2_sns_outputs\n",
    "            enc2_states = enc2_sns_states\n",
    "            dec_inputs = [dec_sns_inputs]\n",
    "            dec_attn_concat = dec_sns_attn\n",
    "            dec_lstm1 = dec_sns_lstm1\n",
    "        elif mode == 'T':\n",
    "            enc2_inputs = [enc2_trend_inputs]\n",
    "            enc2_outputs = enc2_trend_outputs\n",
    "            enc2_states = enc2_trend_states\n",
    "            dec_inputs = [dec_trend_inputs]\n",
    "            dec_attn_concat = dec_trend_attn\n",
    "            dec_lstm1 = dec_trend_lstm1\n",
    "        elif mode == 'NS':\n",
    "            enc2_inputs = [enc2_news_inputs, enc2_sns_inputs]\n",
    "            dec_inputs = [dec_news_inputs, dec_sns_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_news_attn, dec_sns_attn])\n",
    "        elif mode == 'ST':\n",
    "            enc2_inputs = [enc2_sns_inputs, enc2_trend_inputs]\n",
    "            dec_inputs = [dec_sns_inputs, dec_trend_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_sns_attn, dec_trend_attn])\n",
    "        elif mode == 'TN':\n",
    "            enc2_inputs = [enc2_trend_inputs, enc2_news_inputs]\n",
    "            dec_inputs = [dec_trend_inputs, dec_news_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_trend_attn, dec_news_attn])\n",
    "        else:\n",
    "            enc2_inputs = [enc2_news_inputs, enc2_sns_inputs, enc2_trend_inputs]\n",
    "            enc2_outputs = [enc2_news_outputs, enc2_sns_outputs, enc2_trend_outputs]\n",
    "            enc2_states = [enc2_news_states, enc2_sns_states, enc2_trend_states]\n",
    "            dec_inputs = [dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_news_attn, dec_sns_attn, dec_trend_attn])\n",
    "            \n",
    "            \n",
    "        dec_dense1 = TimeDistributed(Dense(DENSE_UNIT, name='dec_dense1'), name='dec_time1')\n",
    "        dec_dense2 = TimeDistributed(Dense(1, name='dec_dense2'), name='dec_time2')\n",
    "        leaky_relu = LeakyReLU(alpha=0.1)\n",
    "        dec_ouputs = dec_dense1(dec_attn_concat)\n",
    "        dec_ouputs = dec_dense2(dec_ouputs)\n",
    "        dec_ouputs = leaky_relu(dec_ouputs)\n",
    "        \n",
    "        if is_sg:\n",
    "            model = Model([enc2_inputs, dec_inputs], dec_ouputs)\n",
    "        else:\n",
    "            model = Model([enc1_inputs, enc2_inputs, dec_inputs], dec_ouputs)\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        self.model = model\n",
    "        \n",
    "        if mode == 'A':\n",
    "            dec_news_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_news_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_news_c_input')\n",
    "            ]\n",
    "            dec_sns_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_sns_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_sns_c_input')\n",
    "            ]\n",
    "            dec_trend_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_trend_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_trend_c_input')\n",
    "            ]\n",
    "            \n",
    "            dec_news_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_news_hidden_input')\n",
    "            dec_sns_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_sns_hidden_input')\n",
    "            dec_trend_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_trend_hidden_input')\n",
    "            \n",
    "            dec_news_output2, dec_news_state_h, dec_news_state_c = dec_news_lstm1(\n",
    "                dec_news_inputs, initial_state=dec_news_state_inputs\n",
    "            )\n",
    "            dec_sns_output2, dec_sns_state_h, dec_sns_state_c = dec_sns_lstm1(\n",
    "                dec_sns_inputs, initial_state=dec_sns_state_inputs\n",
    "            )\n",
    "            dec_trend_output2, dec_trend_state_h, dec_trend_state_c = dec_trend_lstm1(\n",
    "                dec_trend_inputs, initial_state=dec_trend_state_inputs\n",
    "            )\n",
    "            dec_news_attn2 = Attention(name='dec_news_attn2')([dec_news_output2, dec_news_hidden_inputs])\n",
    "            dec_sns_attn2 = Attention(name='dec_sns_attn2')([dec_sns_output2, dec_sns_hidden_inputs])\n",
    "            dec_trend_attn2 = Attention(name='dec_trend_attn2')([dec_trend_output2, dec_trend_hidden_inputs])\n",
    "            \n",
    "            dec_attn_concat2 = Concatenate(name='dec_attn_concat2')([\n",
    "                dec_news_attn2, dec_sns_attn2, dec_trend_attn2\n",
    "            ])\n",
    "            dec_outputs2 = dec_dense1(dec_attn_concat2)\n",
    "            dec_outputs2 = dec_dense2(dec_outputs2)\n",
    "            dec_outputs2 = leaky_relu(dec_outputs2)\n",
    "            \n",
    "            dec_inputs = [dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "            dec_hidden_inputs = [dec_news_hidden_inputs, dec_sns_hidden_inputs, dec_trend_hidden_inputs]\n",
    "            \n",
    "            self.decoder_model = Model([dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "                                      + [dec_news_hidden_inputs, dec_sns_hidden_inputs, dec_trend_hidden_inputs]\n",
    "                                      + [dec_news_state_inputs, dec_sns_state_inputs, dec_trend_state_inputs],\n",
    "                                      [dec_outputs2] \n",
    "                                       + [[dec_news_state_h, dec_news_state_c],\n",
    "                                          [dec_sns_state_h, dec_sns_state_c],\n",
    "                                          [dec_trend_state_h, dec_trend_state_c]])\n",
    "        else:\n",
    "            dec_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_c_input')\n",
    "            ]\n",
    "\n",
    "            dec_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_hidden_input')\n",
    "\n",
    "            dec_outputs2, dec_state_h, dec_state_c = dec_lstm1(dec_inputs[0],\n",
    "                                                                initial_state=dec_state_inputs)\n",
    "            dec_attn2 = Attention(name='dec_test_attn2')([dec_outputs2, dec_hidden_inputs])\n",
    "            dec_outputs2 = dec_dense1(dec_attn2)\n",
    "            dec_outputs2 = dec_dense2(dec_outputs2)\n",
    "            dec_outputs2 = leaky_relu(dec_outputs2)\n",
    "            \n",
    "            self.decoder_model = Model([dec_inputs] + [dec_hidden_inputs] + dec_state_inputs,\n",
    "                                   [dec_outputs2] + [dec_state_h, dec_state_c])\n",
    "        \n",
    "        if is_sg:\n",
    "          self.encoder_model = Model([enc2_inputs], [enc2_outputs] + enc2_states)\n",
    "        else:\n",
    "          self.encoder_model = Model([enc1_inputs, enc2_inputs], [enc2_outputs] + enc2_states)\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        if not self.is_sg:\n",
    "          self.encoder_model.get_layer('enc1_lstm1').set_weights(\n",
    "              self.model.get_layer('enc1_lstm1').get_weights())\n",
    "        self.decoder_model.get_layer('dec_time1').set_weights(\n",
    "            self.model.get_layer('dec_time1').get_weights())\n",
    "        self.decoder_model.get_layer('dec_time2').set_weights(\n",
    "            self.model.get_layer('dec_time2').get_weights())\n",
    "        \n",
    "        if self.mode == 'N':\n",
    "            web_types = ['news']\n",
    "        elif self.mode == 'S':\n",
    "            web_types = ['sns']\n",
    "        elif self.mode == 'T':\n",
    "            web_types = ['trend']\n",
    "        else:\n",
    "            web_types = ['news', 'sns', 'trend']\n",
    "        \n",
    "        for target_name in web_types:\n",
    "            self.encoder_model.get_layer('enc2_' + target_name + '_lstm1').set_weights(\n",
    "                self.model.get_layer('enc2_' + target_name + '_lstm1').get_weights())\n",
    "            self.decoder_model.get_layer('dec_' + target_name + '_lstm1').set_weights(\n",
    "                self.model.get_layer('dec_' + target_name + '_lstm1').get_weights())\n",
    "            \n",
    "        \n",
    "    def transform_data(self, truth, news_data, sns_data, trends_data, \n",
    "                       news_lag, sns_lag, trends_lag,\n",
    "                       year_week, look_back, look_ahead):\n",
    "        \n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _encoder1(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                tmp1 = truth.loc[week-look_back:week-1,\n",
    "                                               [str(y) for y in range(2011, 2017)]]\n",
    "                tmp2 = truth.loc[week-look_back:week-1, '20'+year]\n",
    "                encoder1_input = pd.concat((tmp1, tmp2), axis=1)\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week,\n",
    "                                     [str(y) for y in range(2010, 2016)]]\n",
    "                tmp2 = truth.loc[:week-1,\n",
    "                                     [str(y) for y in range(2011, 2017)]]\n",
    "                tmp2.columns = [str(y) for y in range(2010, 2016)]\n",
    "                tmp2 = pd.concat((tmp1, tmp2))\n",
    "\n",
    "                tmp3 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp4 = truth.loc[:week-1, '20'+year]\n",
    "                tmp4.columns = [['20'+str(int(year)-1)]]\n",
    "                tmp4 = pd.concat((tmp3, tmp4))\n",
    "                encoder1_input = pd.concat((tmp2, tmp4), axis=1)\n",
    "            return encoder1_input.values\n",
    "        \n",
    "        def _encoder2(year_week, look_back):\n",
    "            if self.mode == 'N':\n",
    "                web_data = [news_data]\n",
    "                web_lag = [news_lag]\n",
    "            elif self.mode == 'S':\n",
    "                web_data = [sns_data]\n",
    "                web_lag = [sns_lag]\n",
    "            elif self.mode == 'T':\n",
    "                web_data = [trends_data]\n",
    "                web_lag = [trends_lag]\n",
    "            else:\n",
    "                web_data = [news_data, sns_data, trends_data]\n",
    "                web_lag = [news_lag, sns_lag, trends_lag]\n",
    "            \n",
    "\n",
    "            encoder2_input = []\n",
    "            for data, lag_df in zip(web_data, web_lag):\n",
    "                tmp = []\n",
    "                for w in lag_df.columns:\n",
    "                    lag = int(lag_df[w]['max_lag'])\n",
    "                    origin_point = data[data['weeks'] == year_week].index[0]\n",
    "                    start_point = origin_point-lag-look_back\n",
    "                    end_point = start_point+look_back-1\n",
    "\n",
    "                    tmp.append(\n",
    "                            data.loc[start_point:end_point, w].apply(\n",
    "                            lambda x: np.random.uniform(1e-6, 1e-4) if x == 0 else x).tolist())\n",
    "                tmp = np.array(tmp).T\n",
    "                encoder2_input.append(tmp)\n",
    "            if len(encoder2_input) == 1:\n",
    "                encoder2_input = tmp\n",
    "                \n",
    "            return encoder2_input\n",
    "        \n",
    "        def _decoder(year_week, look_ahead):\n",
    "            year, week = year_week.split('-')\n",
    "            max_week = 52\n",
    "            week = int(week)\n",
    "            if week > 1:\n",
    "                week = week - 1\n",
    "            else:\n",
    "                week = 52\n",
    "                year = str(int(year) - 1)\n",
    "\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "\n",
    "            ground_truth = ground_truth.values.reshape(-1, 1)\n",
    "            \n",
    "            if self.mode == 'N':\n",
    "                web_data = [news_data]\n",
    "                web_lag = [news_lag]\n",
    "            elif self.mode == 'S':\n",
    "                web_data = [sns_data]\n",
    "                web_lag = [sns_lag]\n",
    "            elif self.mode == 'T':\n",
    "                web_data = [trends_data]\n",
    "                web_lag = [trends_lag]\n",
    "            else:\n",
    "                web_data = [news_data, sns_data, trends_data]\n",
    "                web_lag = [news_lag, sns_lag, trends_lag]\n",
    "            \n",
    "            decoder_input = []\n",
    "            \n",
    "            for data, lag_df in zip(web_data, web_lag):\n",
    "                tmp = []\n",
    "                for i, w in enumerate(list(lag_df.columns)):\n",
    "                    lag = int(lag_df[w]['max_lag'])\n",
    "                    origin_point = data[data['weeks'] == year_week].index[0]\n",
    "                    start_point = origin_point-lag\n",
    "                    end_point = start_point+look_ahead-1 if lag >= look_ahead else origin_point\n",
    "\n",
    "                    target_list = data.loc[start_point:end_point, w].tolist()\n",
    "                    random_pad = np.random.uniform(1e-6, 1e-4, (look_ahead,))\n",
    "                    for idx in range(len(target_list)):\n",
    "                        if target_list[idx] == 0:\n",
    "                            continue\n",
    "                        random_pad[idx] = target_list[idx]\n",
    "\n",
    "                    tmp.append(random_pad)\n",
    "                tmp = np.array(tmp).T\n",
    "                tmp = np.concatenate([tmp, ground_truth], axis=-1)\n",
    "                decoder_input.append(tmp)\n",
    "            if len(decoder_input) == 1:\n",
    "                decoder_input = tmp\n",
    "            \n",
    "            return decoder_input\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        encoder1_input = _encoder1(year, week, look_back)\n",
    "        encoder2_input = _encoder2(year_week, look_back)\n",
    "        decoder_input = _decoder(year_week, look_ahead)      \n",
    "\n",
    "        return encoder1_input, encoder2_input, decoder_input, ground_truth    \n",
    "                \n",
    "        \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      news_data=None, sns_data=None, trends_data=None,\n",
    "                      news_lag=None, sns_lag=None, trends_lag=None,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        enc1_inputs = []\n",
    "        enc2_inputs = []\n",
    "        dec_inputs = []\n",
    "        ground_truth = []\n",
    "        \n",
    "        enc2_news_inputs = []\n",
    "        enc2_sns_inputs = []\n",
    "        enc2_trend_inputs = []\n",
    "        dec_news_inputs = []\n",
    "        dec_sns_inputs = []\n",
    "        dec_trend_inputs = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            e1, e2, d, gt = self.transform_data(truth, \n",
    "                                                news_data, sns_data, trends_data,\n",
    "                                                news_lag, sns_lag, trends_lag,\n",
    "                                                aw, look_back, look_ahead)\n",
    "            \n",
    "            enc1_inputs.append(e1)\n",
    "            ground_truth.append(gt)\n",
    "            \n",
    "            if self.mode == 'A':\n",
    "                enc2_news_inputs.append(e2[0])\n",
    "                enc2_sns_inputs.append(e2[1])\n",
    "                enc2_trend_inputs.append(e2[2])\n",
    "                dec_news_inputs.append(d[0])\n",
    "                dec_sns_inputs.append(d[1])\n",
    "                dec_trend_inputs.append(d[2])\n",
    "            else:\n",
    "                enc2_inputs.append(e2)\n",
    "                dec_inputs.append(d)\n",
    "            \n",
    "            \n",
    "        if self.mode == 'A':\n",
    "            x = [enc1_inputs,\n",
    "                 enc2_news_inputs, enc2_sns_inputs, enc2_trend_inputs,\n",
    "                 dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "        else:\n",
    "          if self.is_sg:\n",
    "            x = [np.concatenate([np.array(enc2_inputs),\n",
    "                                 np.array(enc1_inputs)[:, :, -1:]], axis=-1),\n",
    "                 dec_inputs]\n",
    "          else:\n",
    "            enc1_inputs = np.array(enc1_inputs)[:, :, -1-self.past_year_num:]\n",
    "            x = [enc1_inputs, enc2_inputs, dec_inputs]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_LSTM():\n",
    "    def __init__(self, **kwargs):\n",
    "        LSTM_UNIT = 64\n",
    "        DENSE_UNIT = 64\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        \n",
    "        inputs = Input(shape=(self.look_back, 1), name='input1')\n",
    "        lstm1 = LSTM(LSTM_UNIT, return_state=True, name='lstm1')\n",
    "        dense1 = Dense(DENSE_UNIT, name='dense1')\n",
    "        dense2 = Dense(1)\n",
    "        leaky_relu = LeakyReLU(alpha=0.1)\n",
    "        \n",
    "        outputs, _, _ = lstm1(inputs)\n",
    "        outputs = dense1(outputs)\n",
    "        outputs = dense2(outputs)\n",
    "        outputs = leaky_relu(outputs)\n",
    "        \n",
    "        model = Model([inputs], outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "    def transform_data(self, truth, year_week, look_back, look_ahead):\n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _inputs(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                inputs = truth.loc[week-look_back:week-1, '20'+year]\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp2 = truth.loc[:week-1, '20'+year]\n",
    "                tmp2.columns = [['20'+str(int(year)-1)]]\n",
    "                inputs = pd.concat((tmp1, tmp2))\n",
    "            return inputs.values.reshape(-1, 1)\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        inputs = _inputs(year, week, look_back)\n",
    "        \n",
    "        return inputs, ground_truth\n",
    "    \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        ground_truth = []\n",
    "        inputs = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            i, gt = self.transform_data(truth, aw, look_back, look_ahead)\n",
    "            inputs.append(i)\n",
    "            ground_truth.append(gt)\n",
    "        \n",
    "        x = [inputs]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEFSI():\n",
    "    def __init__(self, **kwargs):\n",
    "        LSTM_UNIT = 64\n",
    "        DENSE_UNIT = 64\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        \n",
    "        x1 = Input(shape=(self.look_back, 1), name='x1')\n",
    "        left_lstm1 = LSTM(LSTM_UNIT, return_sequences=True, return_state=True, name='left_lstm1')\n",
    "        left_lstm2 = LSTM(LSTM_UNIT, return_state=True, name='left_lstm2')\n",
    "        left_dense1 = Dense(DENSE_UNIT, name='left_dense1')\n",
    "        \n",
    "        left_outputs, _, _ = left_lstm1(x1)\n",
    "        left_outputs, _, _ = left_lstm2(left_outputs)\n",
    "        left_outputs = left_dense1(left_outputs)\n",
    "        \n",
    "        x2 = Input(shape=(self.look_back, 1), name='x2')\n",
    "        right_lstm1 = LSTM(LSTM_UNIT, return_state=True, name='right_lsmt1')\n",
    "        right_dense1 = Dense(DENSE_UNIT, )\n",
    "        \n",
    "        right_outputs, _, _ = right_lstm1(x2)\n",
    "        right_outputs = right_dense1(right_outputs)\n",
    "        \n",
    "        merge = Average(name='merge')([left_outputs, right_outputs])\n",
    "        merge_dense = Dense(1, name='merge_dense')\n",
    "        outputs = merge_dense(merge)\n",
    "        \n",
    "        model = Model([x1, x2], outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "    def transform_data(self, truth, year_week, look_back, look_ahead):\n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _inputs(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                x1 = truth.loc[week-look_back:week-1, '20'+year]\n",
    "                x2 = truth.loc[week-look_back:week-1, '20'+str(int(year)-1)]\n",
    "                inputs = [x1.values.reshape(-1, 1), x2.values.reshape(-1, 1)]\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp2 = truth.loc[:week-1, '20'+year]\n",
    "                tmp2.columns = [['20'+str(int(year)-1)]]\n",
    "                x1 = pd.concat((tmp1, tmp2)).values\n",
    "\n",
    "                tmp3 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-2)]\n",
    "                tmp4 = truth.loc[:week-1, '20'+str(int(year)-1)]\n",
    "                tmp4.columns = [['20'+str(int(year)-2)]]\n",
    "                x2 = pd.concat((tmp3, tmp4)).values\n",
    "                inputs = [x1.reshape(-1, 1), x2.reshape(-1, 1)]\n",
    "            return inputs\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        inputs = _inputs(year, week, look_back)\n",
    "        \n",
    "        return inputs, ground_truth\n",
    "    \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        ground_truth = []\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            i, gt = self.transform_data(truth, aw, look_back, look_ahead)\n",
    "            x1.append(i[0])\n",
    "            x2.append(i[1])\n",
    "            ground_truth.append(gt)\n",
    "        \n",
    "        x = [x1, x2]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self, **kwargs):\n",
    "        ENC_LSTM_UNIT = 64\n",
    "        DEC_LSTM_UNIT = 128\n",
    "        DROPOUT = 0.2\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        \n",
    "        enc_inputs = Input(shape=(self.look_back, 1), name='enc_input')\n",
    "        enc_lstm1 = Bidirectional(LSTM(ENC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                                       dropout=DROPOUT, name='enc_lstm1'), name='enc_bilstm1')\n",
    "        enc_lstm2 = Bidirectional(LSTM(ENC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                                       dropout=DROPOUT, name='enc_lstm2'), name='enc_bilstm2')\n",
    "        enc_lstm3 = Bidirectional(LSTM(ENC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                                       dropout=DROPOUT, name='enc_lstm3'), name='enc_bilstm3')\n",
    "        enc_outputs, _, _, _, _ = enc_lstm1(enc_inputs)\n",
    "        enc_outputs, _, _, _, _ = enc_lstm2(enc_outputs)\n",
    "        enc_outputs, fh, fc, bh, bc = enc_lstm3(enc_outputs)\n",
    "        \n",
    "        state_h = Concatenate(name='enc_h_concat')([fh, bh])\n",
    "        state_c = Concatenate(name='enc_c_concat')([fc, bc])\n",
    "        \n",
    "        dec_inputs = Input(shape=(None, 1), name='dec_input')\n",
    "        dec_lstm1 = LSTM(DEC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                         dropout=DROPOUT, name='dec_lstm1')\n",
    "        dec_outputs, _, _ = dec_lstm1(dec_inputs, initial_state=[state_h, state_c])\n",
    "        \n",
    "        dec_attn = Attention(name='dec_attn')([dec_outputs, enc_outputs])\n",
    "        dec_concat = Concatenate(name='dec_concat')([dec_outputs, dec_attn])\n",
    "        dec_dense1 = TimeDistributed(Dense(1, name='dense1'), name='time1')\n",
    "        outputs = dec_dense1(dec_concat)\n",
    "        \n",
    "        model = Model([enc_inputs, dec_inputs], outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        dec_state_inputs = [\n",
    "            Input(shape=(DEC_LSTM_UNIT,), name='dec_h_input'),\n",
    "            Input(shape=(DEC_LSTM_UNIT,), name='dec_c_input')\n",
    "        ]\n",
    "        dec_hidden_inputs = Input(shape=(self.look_back, DEC_LSTM_UNIT), name='dec_hidden_input')\n",
    "        \n",
    "        dec_outputs2, dec_state_h, dec_state_c = dec_lstm1(dec_inputs, \n",
    "                                                          initial_state=dec_state_inputs)\n",
    "        dec_attn2 = Attention(name='dec_test_attn2')([dec_outputs2, dec_hidden_inputs])\n",
    "        dec_concat2 = Concatenate(name='dec_test_concat2')([dec_outputs2, dec_attn2])\n",
    "        dec_outputs2 = dec_dense1(dec_concat2)\n",
    "        \n",
    "        self.encoder_model = Model(enc_inputs, [enc_outputs, state_h, state_c])\n",
    "        self.decoder_model = Model([dec_inputs] + [dec_hidden_inputs] + dec_state_inputs,\n",
    "                                   [dec_outputs2] + [dec_state_h, dec_state_c])\n",
    "        \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        self.encoder_model.get_layer('enc_bilstm1').set_weights(\n",
    "            self.model.get_layer('enc_bilstm1').get_weights())\n",
    "        self.encoder_model.get_layer('enc_bilstm2').set_weights(\n",
    "            self.model.get_layer('enc_bilstm2').get_weights())\n",
    "        self.encoder_model.get_layer('enc_bilstm3').set_weights(\n",
    "            self.model.get_layer('enc_bilstm3').get_weights())\n",
    "        self.decoder_model.get_layer('dec_lstm1').set_weights(\n",
    "            self.model.get_layer('dec_lstm1').get_weights())\n",
    "        self.decoder_model.get_layer('time1').set_weights(\n",
    "            self.model.get_layer('time1').get_weights())  \n",
    "        \n",
    "    def transform_data(self, truth, year_week, look_back, look_ahead):\n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _inputs(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                inputs = truth.loc[week-look_back:week-1, '20'+year]\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp2 = truth.loc[:week-1, '20'+year]\n",
    "                tmp2.columns = [['20'+str(int(year)-1)]]\n",
    "                inputs = pd.concat((tmp1, tmp2))\n",
    "            return inputs.values.reshape(-1, 1)\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        inputs = _inputs(year, week, look_back)\n",
    "        inputs = [inputs, np.concatenate([inputs[-1, :].reshape(-1, 1), ground_truth[:-1, :]])]\n",
    "        \n",
    "        return inputs, ground_truth        \n",
    "    \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        ground_truth = []\n",
    "        enc_inputs = []\n",
    "        dec_inputs = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            i, gt = self.transform_data(truth, aw, look_back, look_ahead)\n",
    "            enc_inputs.append(i[0])\n",
    "            dec_inputs.append(i[1])\n",
    "            ground_truth.append(gt)\n",
    "        \n",
    "        x = [enc_inputs, dec_inputs]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "\n",
    "look_back = 8\n",
    "look_ahead = 10\n",
    "past_year_num = 6\n",
    "\n",
    "max_lag = 14\n",
    "collect_err = 22\n",
    "\n",
    "test_split_size = 0.65\n",
    "val_split_size = 0.8\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_by_num_words = {\n",
    "    'US': {},\n",
    "    'AU': {},\n",
    "    'KR': {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# US_trends MEDIF 적용\n",
    "epochs = 100\n",
    "is_sg = False\n",
    "sg_path = '_2y' #if is_sg else ''\n",
    "\n",
    "US_perform_by_model = {}\n",
    "US_prd_by_model = {}\n",
    "\n",
    "for i in [1]:\n",
    "#     perform_by_num_words['US'][i] = {}\n",
    "    mode = 'T'\n",
    "\n",
    "    mode_name = 'ME-' + mode + '(US)'\n",
    "    mode_name = mode_name+'(2Y)' #if is_sg else mode_name\n",
    "    \n",
    "    US_perform_by_model[mode_name] = {}\n",
    "    print('mode:', mode_name)\n",
    "    clear_session()\n",
    "    \n",
    "    target_max_lag = US_trends_max_lag.T.sort_values(by='coef', ascending=False).head(i).T\n",
    "    print(i, target_max_lag.columns)\n",
    "    trend_word_num = len(target_max_lag.columns)\n",
    "    trend_word_num = trend_word_num+1 if is_sg else trend_word_num\n",
    "\n",
    "    me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                          past_year_num=0,\n",
    "                          trend_word_num=trend_word_num,\n",
    "                          mode=mode,\n",
    "                          is_sg=is_sg)\n",
    "\n",
    "    available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "    x, y, val_size, test_size = me.generate_data(CDC_norm, available_weeks,\n",
    "                                                 look_back, look_ahead,\n",
    "                                                 test_split_size=test_split_size,\n",
    "                                                 val_split_size=val_split_size,\n",
    "                                                 trends_data=US_trends_norm,\n",
    "                                                 trends_lag=target_max_lag)\n",
    "\n",
    "    x_train, x_val, x_test, x_total = x\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "    cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "    history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                      callbacks=[cp], verbose=0)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    min_epoch = 90\n",
    "    target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                    min(history.history['val_loss'][min_epoch:]))\n",
    "    best_path = './models/v2/US_me_' + mode + '_lb' + str(look_back) + sg_path + '_best.h5'\n",
    "    print(target_epoch, best_path)\n",
    "    shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "    test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=0,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode, \n",
    "                              is_sg=is_sg)\n",
    "    test_me.load_model(best_path)\n",
    "\n",
    "    start_week = int(available_weeks[0].split('-')[1])\n",
    "    end_week = int(available_weeks[-1].split('-')[1])\n",
    "    target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                    + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "    prd_total = []\n",
    "\n",
    "    for target_idx in range(90-1):\n",
    "        if is_sg:\n",
    "          enc_inputs = [x_total[0][target_idx:target_idx+1]]\n",
    "          dec_input = x_total[1][target_idx, 0, :].reshape(1, 1, x_total[1].shape[2])\n",
    "        else:\n",
    "          enc_inputs = [x_total[0][target_idx:target_idx+1],\n",
    "                        x_total[1][target_idx:target_idx+1]]\n",
    "          dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "          \n",
    "        enc_out, enc_h, enc_c = test_me.encoder_model.predict(enc_inputs)\n",
    "\n",
    "        tmp = []\n",
    "        for ahead in range(look_ahead):\n",
    "            prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "            prd_val = prd[0, 0, 0]\n",
    "            if prd_val > 0:\n",
    "                tmp.append([prd_val])\n",
    "            else:\n",
    "                tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "            if ahead < look_ahead - 1:\n",
    "              if is_sg:\n",
    "                dec_input = x_total[1][target_idx, ahead+1, :].reshape(1, 1, x_total[1].shape[2])\n",
    "              else:\n",
    "                dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "              dec_input[0, 0, -1] = prd_val\n",
    "            enc_h, enc_c = dec_h, dec_c\n",
    "        prd_total.append(tmp)\n",
    "    prd_total = np.array(prd_total)\n",
    "    US_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "    show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "    prd_train = prd_total[:val_size]\n",
    "    prd_val = prd_total[val_size:test_size]\n",
    "    prd_test = prd_total[test_size:]\n",
    "\n",
    "    inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "    inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                           inverse_prd_total[:val_size], look_ahead)\n",
    "    rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                     inverse_prd_total[val_size:test_size], look_ahead)\n",
    "    rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                        inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "    for ahead in range(look_ahead):\n",
    "        ah = str(ahead+1)\n",
    "        US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "        US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "        US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "        US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "        US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "        US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "        US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "        US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "        US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "        US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "        US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "        US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    perform_by_num_words['US'][i]['perform'][mode_name] = US_perform_by_model[mode_name].copy()\n",
    "    perform_by_num_words['US'][i]['prd'][mode_name] = prd_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([US_perform, pd.DataFrame(US_perform_by_model).T])[\n",
    "  ['total_%s_%s' % (ev, i) for i in [1, 2, 3] for ev in ['rmse', 'mape', 'corr']]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic_LSTM\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# US_perform_by_model = {}\n",
    "mode_name = 'Basic-LSTM(US)'\n",
    "US_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = basic_lstm.generate_data(CDC_norm, available_weeks,\n",
    "                                                  look_back, 1, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/basic_lstm_{epoch}.h5')\n",
    "\n",
    "history = basic_lstm.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 10\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/US_basic_lstm_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/basic_lstm_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "test_basic_lstm.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_basic_lstm.model.predict(left_shift)[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1.0 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "US_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFSI\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "# US_perform_by_model = {}\n",
    "mode_name = 'DEFSI(US)'\n",
    "US_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = defsi.generate_data(CDC_norm, available_weeks,\n",
    "                                              look_back, 1, \n",
    "                                              test_split_size=test_split_size,\n",
    "                                              val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/defsi_{epoch}.h5')\n",
    "\n",
    "history = defsi.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/US_defsi_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/defsi_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "test_defsi.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_defsi.model.predict([left_shift, np.array([x_total[1][target_idx+1]])])[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "US_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seq2Seq\n",
    "\n",
    "# US_perform_by_model = {}\n",
    "mode_name = 'Seq2Seq(US)'\n",
    "US_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "\n",
    "available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = seq2seq.generate_data(CDC_norm, available_weeks,\n",
    "                                                  look_back, look_ahead, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/seq2seq_{epoch}.h5')\n",
    "\n",
    "history = seq2seq.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "\n",
    "best_path = './models/v2/US_seq2seq_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/seq2seq_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "test_seq2seq.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "\n",
    "for target_idx in range(90-1):\n",
    "    enc_out, enc_h, enc_c = test_seq2seq.encoder_model.predict(np.array([x_total[0][target_idx]]))\n",
    "\n",
    "    dec_input = x_total[1][target_idx][0].reshape(-1, 1)\n",
    "\n",
    "    tmp = []\n",
    "    for ahead in range(look_ahead):\n",
    "        prd, dec_h, dec_c = test_seq2seq.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "        prd_val = prd[0, 0]\n",
    "        if prd[0, 0] > 0:\n",
    "            tmp.append(prd_val)\n",
    "        else:\n",
    "            tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "        dec_input = prd[0, 0]\n",
    "        enc_h, enc_c = dec_h, dec_c\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "US_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_perform = pd.concat([US_perform, pd.DataFrame(US_perform_by_model).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US_perform = pd.DataFrame(US_perform_by_model).T\n",
    "US_perform.to_csv('./data/v2/US_perform6.csv', encoding='utf8')\n",
    "US_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/US_predict4.pkl', 'wb') as f:\n",
    "    pickle.dump(US_prd_by_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AU_trends MEDIF 적용\n",
    "epochs = 100\n",
    "is_sg = False\n",
    "# sg_path = '_sg' if is_sg else ''\n",
    "sg_path = '_2y'\n",
    "\n",
    "AU_perform_by_model = {}\n",
    "AU_prd_by_model = {}\n",
    "\n",
    "for i in [1]:\n",
    "#     perform_by_num_words['AU'][i] = {}\n",
    "    mode = 'T'\n",
    "\n",
    "    mode_name = 'ME-' + mode + '(AU)'\n",
    "    mode_name = mode_name+'(2Y)' # if is_sg else mode_name\n",
    "    \n",
    "    AU_perform_by_model[mode_name] = {}\n",
    "    print('mode:', mode_name)\n",
    "    clear_session()\n",
    "    \n",
    "    target_max_lag = AU_trends_max_lag.T.sort_values(by='coef', ascending=False).head(i).T\n",
    "    trend_word_num = len(target_max_lag.columns)\n",
    "    trend_word_num = trend_word_num+1 if is_sg else trend_word_num\n",
    "\n",
    "    me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                          past_year_num=5,\n",
    "                          trend_word_num=trend_word_num,\n",
    "                          mode=mode,\n",
    "                          is_sg=is_sg)\n",
    "\n",
    "    available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "    x, y, val_size, test_size = me.generate_data(ASPREN_norm, available_weeks,\n",
    "                                                 look_back, look_ahead,\n",
    "                                                 test_split_size=test_split_size,\n",
    "                                                 val_split_size=val_split_size,\n",
    "                                                 trends_data=AU_trends_norm,\n",
    "                                                 trends_lag=target_max_lag)\n",
    "\n",
    "    x_train, x_val, x_test, x_total = x\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "    cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "    history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                      callbacks=[cp], verbose=0)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    min_epoch = 45\n",
    "    target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                    min(history.history['val_loss'][min_epoch:]))\n",
    "    best_path = './models/v2/AU_me_' + mode + '_lb' + str(look_back) + sg_path + '_best.h5'\n",
    "    print(target_epoch, best_path)\n",
    "    shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "    test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=5,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode,\n",
    "                              is_sg=is_sg)\n",
    "    test_me.load_model(best_path)\n",
    "\n",
    "    start_week = int(available_weeks[0].split('-')[1])\n",
    "    end_week = int(available_weeks[-1].split('-')[1])\n",
    "    target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                    + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "    prd_total = []\n",
    "\n",
    "    for target_idx in range(90-1):\n",
    "        if is_sg:\n",
    "            enc_inputs = [x_total[0][target_idx:target_idx+1]]\n",
    "            dec_input = x_total[1][target_idx, 0, :].reshape(1, 1, x_total[1].shape[2])\n",
    "        else:\n",
    "            enc_inputs = [x_total[0][target_idx:target_idx+1],\n",
    "                          x_total[1][target_idx:target_idx+1]]\n",
    "            dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "\n",
    "        enc_out, enc_h, enc_c = test_me.encoder_model.predict(enc_inputs)\n",
    "\n",
    "        tmp = []\n",
    "        for ahead in range(look_ahead):\n",
    "            prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "            prd_val = prd[0, 0, 0]\n",
    "            if prd_val > 0:\n",
    "                tmp.append([prd_val])\n",
    "            else:\n",
    "                tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "            if ahead < look_ahead - 1:\n",
    "              if is_sg:\n",
    "                dec_input = x_total[1][target_idx, ahead+1, :].reshape(1, 1, x_total[1].shape[2])\n",
    "              else:\n",
    "                dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "              dec_input[0, 0, -1] = prd_val\n",
    "            enc_h, enc_c = dec_h, dec_c\n",
    "        prd_total.append(tmp)\n",
    "    prd_total = np.array(prd_total)\n",
    "    AU_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "    show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "    prd_train = prd_total[:val_size]\n",
    "    prd_val = prd_total[val_size:test_size]\n",
    "    prd_test = prd_total[test_size:]\n",
    "\n",
    "    inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "    inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                           inverse_prd_total[:val_size], look_ahead)\n",
    "    rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                     inverse_prd_total[val_size:test_size], look_ahead)\n",
    "    rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                        inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "    for ahead in range(look_ahead):\n",
    "        ah = str(ahead+1)\n",
    "        AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "        AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "        AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "        AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "        AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "        AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "        AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "        AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "        AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "        AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "        AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "        AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    perform_by_num_words['AU'][i]['perform'][mode_name] = AU_perform_by_model[mode_name].copy()\n",
    "    perform_by_num_words['AU'][i]['prd'][mode_name] = prd_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([AU_perform, pd.DataFrame(AU_perform_by_model).T])[['total_%s_%s' % (ev, i) for i in [1,2,3] for ev in ['rmse', 'mape', 'corr']]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic_LSTM\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# AU_perform_by_model = {}\n",
    "mode_name = 'Basic-LSTM(AU)'\n",
    "AU_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = basic_lstm.generate_data(ASPREN_norm, available_weeks,\n",
    "                                                  look_back, 1, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/basic_lstm_{epoch}.h5')\n",
    "\n",
    "history = basic_lstm.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 10\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/AU_basic_lstm_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/basic_lstm_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "test_basic_lstm.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_basic_lstm.model.predict(left_shift)[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1.0 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "AU_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFSI\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "# AU_perform_by_model = {}\n",
    "mode_name = 'DEFSI(AU)'\n",
    "AU_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = defsi.generate_data(ASPREN_norm, available_weeks,\n",
    "                                              look_back, 1, \n",
    "                                              test_split_size=test_split_size,\n",
    "                                              val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/defsi_{epoch}.h5')\n",
    "\n",
    "history = defsi.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/AU_defsi_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/defsi_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "test_defsi.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_defsi.model.predict([left_shift, np.array([x_total[1][target_idx+1]])])[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "AU_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seq2Seq\n",
    "\n",
    "# AU_perform_by_model = {}\n",
    "mode_name = 'Seq2Seq(AU)'\n",
    "AU_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "\n",
    "available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = seq2seq.generate_data(ASPREN_norm, available_weeks,\n",
    "                                                  look_back, look_ahead, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/seq2seq_{epoch}.h5')\n",
    "\n",
    "history = seq2seq.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "\n",
    "best_path = './models/v2/AU_seq2seq_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/seq2seq_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "test_seq2seq.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "\n",
    "for target_idx in range(90-1):\n",
    "    enc_out, enc_h, enc_c = test_seq2seq.encoder_model.predict(np.array([x_total[0][target_idx]]))\n",
    "\n",
    "    dec_input = x_total[1][target_idx][0].reshape(-1, 1)\n",
    "\n",
    "    tmp = []\n",
    "    for ahead in range(look_ahead):\n",
    "        prd, dec_h, dec_c = test_seq2seq.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "        prd_val = prd[0, 0]\n",
    "        if prd[0, 0] > 0:\n",
    "            tmp.append(prd_val)\n",
    "        else:\n",
    "            tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "        dec_input = prd[0, 0]\n",
    "        enc_h, enc_c = dec_h, dec_c\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "AU_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AU_perform = pd.concat([AU_perform, pd.DataFrame(AU_perform_by_model).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AU_perform = pd.DataFrame(AU_perform_by_model).T\n",
    "AU_perform.to_csv('./data/v2/AU_perform6.csv', encoding='utf8')\n",
    "AU_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/AU_predict4.pkl', 'wb') as f:\n",
    "    pickle.dump(AU_prd_by_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , look_back param check\n",
    "\n",
    "KR_perform_by_lookback = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for look_back in range(3, 11):\n",
    "    print('look back:', look_back)\n",
    "    KR_perform_by_lookback[look_back] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_lookback[look_back]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_lookback[look_back]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_lookback.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_lookback, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "for look_back in range(3, 11):\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_lookback[look_back]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_compare = {}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    lb_compare[ev] = []\n",
    "    for lb in range(3, 11):\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_lookback[lb]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        lb_compare[ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "\n",
    "lb_compare = pd.DataFrame(lb_compare, index=[_ for _ in range(3, 11)])\n",
    "lb_compare['rmse(improve)'] = lb_compare['rmse'].apply(lambda x: (lb_compare['rmse'].max() - x) / lb_compare['rmse'].max())\n",
    "lb_compare['mape(improve)'] = lb_compare['mape'].apply(lambda x: (lb_compare['mape'].max() - x) / lb_compare['mape'].max())\n",
    "lb_compare['corr(improve)'] = lb_compare['corr'].apply(lambda x: (x - lb_compare['corr'].min()) / lb_compare['corr'].min())\n",
    "lb_compare['improve(mean)'] = lb_compare[['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "lb_compare.to_csv('./data/v2/KR_perform_by_lookback.csv', encoding='utf8')\n",
    "lb_compare.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , layer_size param check\n",
    "\n",
    "KR_perform_by_layersize = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for layer_size in [32, 64, 128, 256, 512]:\n",
    "    print('layer_size:', layer_size)\n",
    "    KR_perform_by_layersize[layer_size] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              layer_size=layer_size,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  layer_size=layer_size,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_layersize[layer_size]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_layersize[layer_size]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_layersize.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_layersize, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "for layer_size in [32, 64, 128, 256, 512]:\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_layersize[layer_size]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_compare = {}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    ls_compare[ev] = []\n",
    "    for ls in [32, 64, 128, 256, 512]:\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_layersize[ls]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        ls_compare[ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "\n",
    "ls_compare = pd.DataFrame(ls_compare, index=[32, 64, 128, 256, 512])\n",
    "ls_compare['rmse(improve)'] = ls_compare['rmse'].apply(lambda x: (ls_compare['rmse'].max() - x) / ls_compare['rmse'].max())\n",
    "ls_compare['mape(improve)'] = ls_compare['mape'].apply(lambda x: (ls_compare['mape'].max() - x) / ls_compare['mape'].max())\n",
    "ls_compare['corr(improve)'] = ls_compare['corr'].apply(lambda x: (x - ls_compare['corr'].min()) / ls_compare['corr'].min())\n",
    "ls_compare['improve(mean)'] = ls_compare[['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "ls_compare.to_csv('./data/v2/KR_perform_by_layersize.csv', encoding='utf8')\n",
    "ls_compare.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , max_lag coef threshold param check\n",
    "\n",
    "KR_perform_by_coef = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for threshold in [0.0, 0.1, 0.2, 0.3, 0.4]:\n",
    "    \n",
    "    KR_news_tmp_max_lag = {}\n",
    "    lag_max = KR_news_lag.max()\n",
    "    lag_max = lag_max[lag_max > threshold]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_news_lag[word].argmax()\n",
    "        val = KR_news_lag.loc[lag, word]\n",
    "        if lag >= 0:\n",
    "            KR_news_tmp_max_lag[word] = (lag, val)\n",
    "    KR_news_tmp_max_lag = pd.DataFrame(KR_news_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_sns_tmp_max_lag = {}\n",
    "    lag_max = KR_sns_lag.max()\n",
    "    lag_max = lag_max[lag_max > threshold]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_sns_lag[word].argmax()\n",
    "        val = KR_sns_lag.loc[lag, word]\n",
    "        if lag >= 0:\n",
    "            KR_sns_tmp_max_lag[word] = (lag, val)\n",
    "    KR_sns_tmp_max_lag = pd.DataFrame(KR_sns_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_trends_tmp_max_lag = {}\n",
    "    lag_max = KR_trends_lag.max()\n",
    "    lag_max = lag_max[lag_max > threshold]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_trends_lag[word].argmax()\n",
    "        val = KR_trends_lag.loc[lag, word]\n",
    "        if lag >= 0:\n",
    "            KR_trends_tmp_max_lag[word] = (lag, val)\n",
    "    KR_trends_tmp_max_lag = pd.DataFrame(KR_trends_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    print('threshold:', threshold)\n",
    "    KR_perform_by_coef[threshold] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_tmp_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_tmp_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_tmp_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_tmp_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_tmp_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_tmp_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_coef[threshold]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_coef[threshold]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_coef.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_coef, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "for threshold in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_coef[threshold]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_compare = {'N': {}, 'S': {}, 'T': {}, 'mean': {}}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    coef_compare['N'][ev] = []\n",
    "    coef_compare['S'][ev] = []\n",
    "    coef_compare['T'][ev] = []\n",
    "    coef_compare['mean'][ev] = []\n",
    "    for coef_threshold in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_coef[coef_threshold]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        n, s, t = tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).values\n",
    "        \n",
    "        coef_compare['N'][ev].append(n)\n",
    "        coef_compare['S'][ev].append(s)\n",
    "        coef_compare['T'][ev].append(t)\n",
    "        coef_compare['mean'][ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "        \n",
    "for mode in ['N', 'S', 'T', 'mean']:\n",
    "    coef_compare[mode] = pd.DataFrame(coef_compare[mode], index=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "    coef_compare[mode]['rmse(improve)'] = coef_compare[mode]['rmse'].apply(lambda x: (coef_compare[mode]['rmse'].max() - x) / coef_compare[mode]['rmse'].max())\n",
    "    coef_compare[mode]['mape(improve)'] = coef_compare[mode]['mape'].apply(lambda x: (coef_compare[mode]['mape'].max() - x) / coef_compare[mode]['mape'].max())\n",
    "    coef_compare[mode]['corr(improve)'] = coef_compare[mode]['corr'].apply(lambda x: (x - coef_compare[mode]['corr'].min()) / coef_compare[mode]['corr'].min())\n",
    "    coef_compare[mode]['improve(mean)'] = coef_compare[mode][['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "    coef_compare[mode].to_csv('./data/v2/KR_perform_by_coef_' + mode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare['mean'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , max_lag lag threshold param check\n",
    "\n",
    "KR_perform_by_lag = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for threshold in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    \n",
    "    KR_news_tmp_max_lag = {}\n",
    "    lag_max = KR_news_lag.max()\n",
    "    lag_max = lag_max[lag_max > 0.5]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_news_lag[word].argmax()\n",
    "        val = KR_news_lag.loc[lag, word]\n",
    "        if lag >= threshold:\n",
    "            KR_news_tmp_max_lag[word] = (lag, val)\n",
    "    KR_news_tmp_max_lag = pd.DataFrame(KR_news_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_sns_tmp_max_lag = {}\n",
    "    lag_max = KR_sns_lag.max()\n",
    "    lag_max = lag_max[lag_max > 0.5]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_sns_lag[word].argmax()\n",
    "        val = KR_sns_lag.loc[lag, word]\n",
    "        if lag >= threshold:\n",
    "            KR_sns_tmp_max_lag[word] = (lag, val)\n",
    "    KR_sns_tmp_max_lag = pd.DataFrame(KR_sns_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_trends_tmp_max_lag = {}\n",
    "    lag_max = KR_trends_lag.max()\n",
    "    lag_max = lag_max[lag_max > 0.5]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_trends_lag[word].argmax()\n",
    "        val = KR_trends_lag.loc[lag, word]\n",
    "        if lag >= threshold:\n",
    "            KR_trends_tmp_max_lag[word] = (lag, val)\n",
    "    KR_trends_tmp_max_lag = pd.DataFrame(KR_trends_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    print('='*5, 'threshold:', threshold, '='*5)\n",
    "    KR_perform_by_lag[threshold] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_tmp_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_tmp_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_tmp_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_tmp_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_tmp_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_tmp_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_lag[threshold]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_lag[threshold]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_lag.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_lag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "for threshold in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_lag[threshold]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_compare = {'N': {}, 'S': {}, 'T': {}, 'mean': {}}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    lag_compare['N'][ev] = []\n",
    "    lag_compare['S'][ev] = []\n",
    "    lag_compare['T'][ev] = []\n",
    "    lag_compare['mean'][ev] = []\n",
    "    for lag_threshold in np.arange(0, 11):\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_lag[lag_threshold]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        n, s, t = tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).values\n",
    "        \n",
    "        lag_compare['N'][ev].append(n)\n",
    "        lag_compare['S'][ev].append(s)\n",
    "        lag_compare['T'][ev].append(t)\n",
    "        lag_compare['mean'][ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "        \n",
    "for mode in ['N', 'S', 'T', 'mean']:\n",
    "    lag_compare[mode] = pd.DataFrame(lag_compare[mode], index=np.arange(0, 11))\n",
    "    lag_compare[mode]['rmse(improve)'] = lag_compare[mode]['rmse'].apply(lambda x: (lag_compare[mode]['rmse'].max() - x) / lag_compare[mode]['rmse'].max())\n",
    "    lag_compare[mode]['mape(improve)'] = lag_compare[mode]['mape'].apply(lambda x: (lag_compare[mode]['mape'].max() - x) / lag_compare[mode]['mape'].max())\n",
    "    lag_compare[mode]['corr(improve)'] = lag_compare[mode]['corr'].apply(lambda x: (x - lag_compare[mode]['corr'].min()) / lag_compare[mode]['corr'].min())\n",
    "    lag_compare[mode]['improve(mean)'] = lag_compare[mode][['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "    lag_compare[mode].to_csv('./data/v2/KR_perform_by_lag_' + mode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_compare['mean'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF 적용\n",
    "epochs = 200\n",
    "is_sg = False\n",
    "sg_path = '_2y' # if is_sg else ''\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "# KR_prd_by_model = {}\n",
    "\n",
    "mode_num_words = {'N': 10, 'S': 6, 'T': 9}\n",
    "#     perform_by_num_words['KR'][i] = {}\n",
    "for mode in ['S']:\n",
    "    i = mode_num_words[mode]\n",
    "    mode_name = 'ME-' + mode\n",
    "    mode_name = mode_name+'(2Y)' # if is_sg else mode_name\n",
    "    \n",
    "    KR_perform_by_model[mode_name] = {}\n",
    "    print('mode:', mode_name)\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    news_target_max_lag = KR_news_max_lag.T.sort_values(by='coef', ascending=False).T\n",
    "    sns_target_max_lag = KR_sns_max_lag.T.sort_values(by='coef', ascending=False).head(6).T\n",
    "    trends_target_max_lag = KR_trends_max_lag.T.sort_values(by='coef', ascending=False).head(9).T\n",
    "\n",
    "    if mode == 'N':\n",
    "        print(i, 'news', news_target_max_lag.columns)\n",
    "    elif mode == 'S':\n",
    "        print(i, 'sns', sns_target_max_lag.columns)\n",
    "    else:\n",
    "        print(i, 'trends', trends_target_max_lag.columns)\n",
    "\n",
    "    news_word_num = len(news_target_max_lag.columns)\n",
    "    news_word_num = news_word_num+1 if is_sg else news_word_num\n",
    "    sns_word_num = len(sns_target_max_lag.columns)\n",
    "    sns_word_num = sns_word_num+1 if is_sg else sns_word_num\n",
    "    trend_word_num = len(trends_target_max_lag.columns)\n",
    "    trend_word_num = trend_word_num+1 if is_sg else trend_word_num\n",
    "\n",
    "    me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                          past_year_num=4,\n",
    "                          news_word_num=news_word_num,\n",
    "                          sns_word_num=sns_word_num,\n",
    "                          trend_word_num=trend_word_num,\n",
    "                          mode=mode,\n",
    "                          is_sg=is_sg)\n",
    "\n",
    "    available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "    x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                 look_back, look_ahead,\n",
    "                                                 test_split_size=test_split_size,\n",
    "                                                 val_split_size=val_split_size,\n",
    "                                                 news_data=KR_news_norm,\n",
    "                                                 news_lag=news_target_max_lag,\n",
    "                                                 sns_data=KR_sns_norm,\n",
    "                                                 sns_lag=sns_target_max_lag,\n",
    "                                                 trends_data=KR_trends_norm,\n",
    "                                                 trends_lag=trends_target_max_lag)\n",
    "\n",
    "    x_train, x_val, x_test, x_total = x\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "    cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "    history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                      callbacks=[cp], verbose=0)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    min_epoch = 100\n",
    "    target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                    min(history.history['val_loss'][min_epoch:]))\n",
    "    best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + sg_path + '_best.h5'\n",
    "    print(target_epoch, best_path)\n",
    "    shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "    test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=4,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode,\n",
    "                              is_sg=is_sg)\n",
    "    test_me.load_model(best_path)\n",
    "\n",
    "    start_week = int(available_weeks[0].split('-')[1])\n",
    "    end_week = int(available_weeks[-1].split('-')[1])\n",
    "    target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                    + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "    prd_total = []\n",
    "\n",
    "    for target_idx in range(90-1):\n",
    "        if mode == 'A':\n",
    "            enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                        [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                         x_total[2][target_idx:target_idx+1],\n",
    "                                                                                         x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "            dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                         x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                         x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "            tmp = []\n",
    "            for ahead in range(look_ahead):\n",
    "                prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                  + [news_states, sns_states, trend_states])\n",
    "                prd_val = prd[0, 0, 0]\n",
    "                if prd_val > 0:\n",
    "                    tmp.append([prd_val])\n",
    "                else:\n",
    "                    tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                if ahead < look_ahead - 1:\n",
    "                    dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                         x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                         x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                    dec_input[0][0, 0, -1] = prd_val\n",
    "                    dec_input[1][0, 0, -1] = prd_val\n",
    "                    dec_input[2][0, 0, -1] = prd_val\n",
    "        else:\n",
    "            if is_sg:\n",
    "              enc_inputs = [x_total[0][target_idx:target_idx+1]]\n",
    "              dec_input = x_total[1][target_idx, 0, :].reshape(1, 1, x_total[1].shape[2])\n",
    "            else:\n",
    "              enc_inputs = [x_total[0][target_idx:target_idx+1],\n",
    "                            x_total[1][target_idx:target_idx+1]]\n",
    "              dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "            enc_out, enc_h, enc_c = test_me.encoder_model.predict(enc_inputs)\n",
    "\n",
    "            tmp = []\n",
    "            for ahead in range(look_ahead):\n",
    "                prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                prd_val = prd[0, 0, 0]\n",
    "                if prd_val > 0:\n",
    "                    tmp.append([prd_val])\n",
    "                else:\n",
    "                    tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                if ahead < look_ahead - 1:\n",
    "                    if is_sg:\n",
    "                      dec_input = x_total[1][target_idx, ahead+1, :].reshape(1, 1, x_total[1].shape[2])\n",
    "                    else:\n",
    "                      dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                    dec_input[0, 0, -1] = prd_val\n",
    "                enc_h, enc_c = dec_h, dec_c\n",
    "        prd_total.append(tmp)\n",
    "    prd_total = np.array(prd_total)\n",
    "    KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "    show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "    prd_train = prd_total[:val_size]\n",
    "    prd_val = prd_total[val_size:test_size]\n",
    "    prd_test = prd_total[test_size:]\n",
    "\n",
    "    inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "    inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                           inverse_prd_total[:val_size], look_ahead)\n",
    "    rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                     inverse_prd_total[val_size:test_size], look_ahead)\n",
    "    rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                        inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "    for ahead in range(look_ahead):\n",
    "        ah = str(ahead+1)\n",
    "        KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "        KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "        KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "        KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "        KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "        KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "        KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "        KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "        KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "        KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "        KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "        KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    perform_by_num_words['KR'][i]['perform'][mode_name] = KR_perform_by_model[mode_name].copy()\n",
    "    perform_by_num_words['KR'][i]['prd'][mode_name] = prd_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([KR_perform, pd.DataFrame(KR_perform_by_model).T])[\n",
    "  ['total_%s_%s' % (ev, i) for i in [5, 8, 10] for ev in ['rmse', 'mape', 'corr']]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "ev = 'rmse'\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['US'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(US)'].values\n",
    "axs[0].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['US'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(US)(SG)'].values\n",
    "axs[0].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['AU'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(AU)'].values\n",
    "axs[1].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['AU'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(AU)(SG)'].values\n",
    "axs[1].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][10]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-N'].values\n",
    "axs[2].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][10]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-N(SG)'].values\n",
    "axs[2].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][6]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-S'].values\n",
    "axs[2].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][6]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-S(SG)'].values\n",
    "axs[2].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T'].values\n",
    "axs[2].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(SG)'].values\n",
    "axs[2].plot(perform_vals, ls='--')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개 국가의 1, 5, 10주 예측 그래프\n",
    "\n",
    "markers = ['.', 's', 'D', 'v', 'h']\n",
    "colors = ['tab:brown', 'tab:purple', 'tab:pink', 'tab:olive', 'tab:blue']\n",
    "line_styles = ['--', '-.', '--', '-', '-']\n",
    "x_labels = ['week\\n(a) ahead-', 'week\\n(b) ahead-', 'week\\n(c) ahead-']\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for col, ev in enumerate(['rmse', 'mape', 'corr']):\n",
    "    target_df = US_perform[[c for c in US_perform if 'total' in c and ev in c]]\n",
    "    for i, m in enumerate(['Basic-LSTM(US)', 'DEFSI(US)', 'Seq2Seq(US)', 'ME-T(US)(SG)', 'ME-T(US)']):\n",
    "        perform_vals = (target_df.T[m].values if 'ME-' not in m \n",
    "                        else pd.DataFrame(perform_by_num_words['US'][1]['perform']).T[\n",
    "                            ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                        ].T[m].values.reshape(-1))\n",
    "        axs[0, col].plot(perform_vals, label='MEDIF-' + m[3:], marker=markers[i], c=colors[i],\n",
    "                ls=line_styles[i])\n",
    "        axs[0, col].set_ylabel(ev.upper())\n",
    "        axs[0, col].set_xlabel('Prediction-period (weeks)')\n",
    "        axs[0, col].set_xticks(range(1, 11, 2))\n",
    "        axs[0, col].set_xticklabels(range(2, 12, 2))\n",
    "        axs[0, col].grid(True, ls=':')\n",
    "axs[0, 1].set_xlabel('Prediction-period (weeks)\\n(a) United States')\n",
    "        \n",
    "for col, ev in enumerate(['rmse', 'mape', 'corr']):\n",
    "    target_df = AU_perform[[c for c in AU_perform if 'total' in c and ev in c]]\n",
    "    for i, m in enumerate(['Basic-LSTM(AU)', 'DEFSI(AU)', 'Seq2Seq(AU)', 'ME-T(AU)(SG)', 'ME-T(AU)']):\n",
    "        perform_vals = (target_df.T[m].values if 'ME-' not in m \n",
    "                        else pd.DataFrame(perform_by_num_words['AU'][1]['perform']).T[\n",
    "                            ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                        ].T[m].values.reshape(-1))\n",
    "        \n",
    "        axs[1, col].plot(perform_vals, label='MEDIF-' + m[3:], marker=markers[i], c=colors[i],\n",
    "                ls=line_styles[i])\n",
    "        axs[1, col].set_ylabel(ev.upper())\n",
    "        axs[1, col].set_xlabel('Prediction-period (weeks)')\n",
    "        axs[1, col].set_xticks(range(1, 11, 2))\n",
    "        axs[1, col].set_xticklabels(range(2, 12, 2))\n",
    "        axs[1, col].grid(True, ls=':')\n",
    "axs[1, 1].set_xlabel('Prediction-period (weeks)\\n(b) Australia')\n",
    "        \n",
    "markers = ['.', 'v', 's', 'p', 'D', 'h', '*']\n",
    "colors = ['tab:brown', 'tab:purple', 'tab:pink', 'tab:red', 'tab:green', 'tab:blue']\n",
    "line_styles = ['--', '-.', '--', '-', '-', '-']\n",
    "for col, ev in enumerate(['rmse', 'mape', 'corr']):\n",
    "    target_df = KR_perform[[c for c in KR_perform if 'total' in c and ev in c]]\n",
    "    for i, m in enumerate(['Basic-LSTM', 'DEFSI', 'Seq2Seq', 'ME-N', 'ME-S', 'ME-T']):\n",
    "        if 'ME-' not in m:\n",
    "            perform_vals = target_df.T[m].values\n",
    "        else:\n",
    "            if m.split('-')[1] == 'N':\n",
    "                perform_vals = target_df.T[m].values\n",
    "            elif m.split('-')[1] == 'S':\n",
    "                perform_vals = pd.DataFrame(perform_by_num_words['KR'][6]['perform']).T[\n",
    "                                ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                                ].T[m].values\n",
    "            else:\n",
    "                perform_vals = pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "                                ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                                ].T[m].values\n",
    "                \n",
    "                \n",
    "#         perform_vals = (target_df.T[m].values if 'ME-' not in m \n",
    "#                         else pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "#                             ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "#                         ].T[m].values)\n",
    "        \n",
    "        axs[2, col].plot(perform_vals, marker=markers[i], c=colors[i],\n",
    "                ls=line_styles[i], label=m)\n",
    "        axs[2, col].set_ylabel(ev.upper())\n",
    "        axs[2, col].set_xlabel('Prediction-period (weeks)')\n",
    "        axs[2, col].set_xticks(range(1, 11, 2))\n",
    "        axs[2, col].set_xticklabels(range(2, 12, 2))\n",
    "        axs[2, col].grid(True, ls=':')\n",
    "axs[2, 1].set_xlabel('Prediction-period (weeks)\\n(c) Korea')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(-0.7, -0.25), ncol=6,\n",
    "          labels=['LSTM', 'DEFSI', 'STS-ATT',\n",
    "                  'Proposed(N)', 'Proposed(S)', 'Proposed(Q)'])\n",
    "# plt.savefig('./image/v3/All_perform_apply_num_words_down_dpi.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/perform_by_num_words2.pkl', 'wb') as f:\n",
    "    pickle.dump(perform_by_num_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic_LSTM\n",
    "\n",
    "epochs = 70\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "mode_name = 'Basic-LSTM'\n",
    "KR_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = basic_lstm.generate_data(KCDC_norm, available_weeks,\n",
    "                                                  look_back, 1, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/basic_lstm_{epoch}.h5')\n",
    "\n",
    "history = basic_lstm.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 30\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/KR_basic_lstm_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/basic_lstm_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "test_basic_lstm.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_basic_lstm.model.predict(left_shift)[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1.0 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFSI\n",
    "\n",
    "test_split_size = 0.65\n",
    "val_split_size = 0.7\n",
    "epochs = 300\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "mode_name = 'DEFSI'\n",
    "KR_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = defsi.generate_data(KCDC_norm, available_weeks,\n",
    "                                              look_back, 1, \n",
    "                                              test_split_size=test_split_size,\n",
    "                                              val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/defsi_{epoch}.h5')\n",
    "\n",
    "history = defsi.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/KR_defsi_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/defsi_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "test_defsi.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_defsi.model.predict([left_shift, np.array([x_total[1][target_idx+1]])])[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seq2Seq\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "mode_name = 'Seq2Seq'\n",
    "KR_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "\n",
    "available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = seq2seq.generate_data(KCDC_norm, available_weeks,\n",
    "                                                  look_back, look_ahead, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/seq2seq_{epoch}.h5')\n",
    "\n",
    "history = seq2seq.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 200\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "\n",
    "best_path = './models/v2/KR_seq2seq_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/seq2seq_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "test_seq2seq.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "\n",
    "for target_idx in range(90-1):\n",
    "    enc_out, enc_h, enc_c = test_seq2seq.encoder_model.predict(np.array([x_total[0][target_idx]]))\n",
    "\n",
    "    dec_input = x_total[1][target_idx][0].reshape(-1, 1)\n",
    "\n",
    "    tmp = []\n",
    "    for ahead in range(look_ahead):\n",
    "        prd, dec_h, dec_c = test_seq2seq.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "        prd_val = prd[0, 0]\n",
    "        if prd[0, 0] > 0:\n",
    "            tmp.append(prd_val)\n",
    "        else:\n",
    "            tmp.append([np.random.uniform(1e-4, 1e-3)])\n",
    "\n",
    "        dec_input = prd[0, 0]\n",
    "        enc_h, enc_c = dec_h, dec_c\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "prd_total = np.array(prd_total)\n",
    "KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform = pd.concat([KR_perform, pd.DataFrame(KR_perform_by_model).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KR_perform = pd.DataFrame(KR_perform_by_model).T\n",
    "KR_perform.to_csv('./data/v2/KR_perform6.csv', encoding='utf8')\n",
    "KR_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KR perform table\n",
    "\n",
    "KR_perform_mini = KR_perform[['total_rmse_1', 'total_mape_1', 'total_corr_1', \n",
    "            'total_rmse_5', 'total_mape_5', 'total_corr_5', \n",
    "            'total_rmse_10', 'total_mape_10', 'total_corr_10', ]].loc[['Basic-LSTM', 'DEFSI', 'Seq2Seq', 'ME-N', 'ME-S', 'ME-T']].round(3)\n",
    "KR_perform_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_predict4.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_prd_by_model, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "py38tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
