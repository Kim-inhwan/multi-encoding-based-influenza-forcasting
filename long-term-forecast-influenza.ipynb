{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Term Forecast Influenza using Web data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from utils.preprocessing import normalize_flu, normalize_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate:\n",
    "    @classmethod\n",
    "    def RMSE(cls, x, y):\n",
    "        x, y = np.array(x).reshape(-1), np.array(y).reshape(-1)\n",
    "        return mean_squared_error(x, y) ** 0.5\n",
    "    \n",
    "    @classmethod\n",
    "    def MAPE(cls, x, y):\n",
    "        x, y = np.array(x).reshape(-1), np.array(y).reshape(-1)\n",
    "        return np.mean(np.abs((y - x) / y)) * 100\n",
    "    \n",
    "    @classmethod\n",
    "    def CORR(cls, x, y):\n",
    "        x, y = np.array(x).reshape(-1), np.array(y).reshape(-1)\n",
    "        return stats.pearsonr(x, y)[0]\n",
    "    \n",
    "    @classmethod\n",
    "    def evaluate(cls, truth, prd, look_ahead):\n",
    "        rmse, mape, corr = [], [], []\n",
    "        for ahead in range(look_ahead):\n",
    "            rmse.append(cls.RMSE(truth[ahead:len(prd)+ahead], prd[:, ahead]))\n",
    "            mape.append(cls.MAPE(truth[ahead:len(prd)+ahead], prd[:, ahead]))\n",
    "            corr.append(cls.CORR(truth[ahead:len(prd)+ahead], prd[:, ahead]))\n",
    "        return rmse, mape, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph(truth, prd, look_ahead, val_size=None, test_size=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.plot(truth)\n",
    "    for i, p in enumerate(prd):\n",
    "        ax1.plot(range(i, i+look_ahead), p)\n",
    "        \n",
    "    ax2.plot(truth)\n",
    "    for ahead in range(look_ahead):\n",
    "        ax2.plot(range(ahead, len(prd)+ahead), \n",
    "                 [p[ahead] for p in prd], label='ahead:' + str(ahead))\n",
    "    ax2.legend()\n",
    "        \n",
    "    if val_size is not None:\n",
    "        ax1.axvline(x=val_size, c='k', ls='--')\n",
    "        ax2.axvline(x=val_size, c='k', ls='--')\n",
    "    if test_size is not None:\n",
    "        ax1.axvline(x=test_size, c='k', ls='--')\n",
    "        ax2.axvline(x=test_size, c='k', ls='--')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US, AU, KR influenza load\n",
    "\n",
    "US_flu = pd.read_csv('./data/US_influenza.csv', index_col=0)\n",
    "AU_flu = pd.read_csv('./data/AU_influenza.csv', index_col=0)\n",
    "KR_flu = pd.read_csv('./data/KR_influenza.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.8284</td>\n",
       "      <td>26.9151</td>\n",
       "      <td>17.6709</td>\n",
       "      <td>49.6368</td>\n",
       "      <td>42.1120</td>\n",
       "      <td>40.7898</td>\n",
       "      <td>19.8859</td>\n",
       "      <td>30.8102</td>\n",
       "      <td>53.5765</td>\n",
       "      <td>36.3095</td>\n",
       "      <td>62.1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.2749</td>\n",
       "      <td>29.1063</td>\n",
       "      <td>15.4340</td>\n",
       "      <td>46.9072</td>\n",
       "      <td>34.6492</td>\n",
       "      <td>39.3762</td>\n",
       "      <td>19.8985</td>\n",
       "      <td>30.1875</td>\n",
       "      <td>55.2437</td>\n",
       "      <td>31.3643</td>\n",
       "      <td>50.6711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.2606</td>\n",
       "      <td>34.9232</td>\n",
       "      <td>16.4762</td>\n",
       "      <td>47.0473</td>\n",
       "      <td>33.1698</td>\n",
       "      <td>41.5076</td>\n",
       "      <td>21.0710</td>\n",
       "      <td>34.0446</td>\n",
       "      <td>63.4528</td>\n",
       "      <td>33.6213</td>\n",
       "      <td>53.5768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.2495</td>\n",
       "      <td>40.0370</td>\n",
       "      <td>16.8430</td>\n",
       "      <td>41.6460</td>\n",
       "      <td>32.4070</td>\n",
       "      <td>38.7899</td>\n",
       "      <td>21.9256</td>\n",
       "      <td>35.2869</td>\n",
       "      <td>70.1811</td>\n",
       "      <td>38.8115</td>\n",
       "      <td>60.2274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.8877</td>\n",
       "      <td>44.3534</td>\n",
       "      <td>18.6354</td>\n",
       "      <td>36.9533</td>\n",
       "      <td>28.8598</td>\n",
       "      <td>34.5794</td>\n",
       "      <td>23.0014</td>\n",
       "      <td>41.0665</td>\n",
       "      <td>77.8090</td>\n",
       "      <td>45.2235</td>\n",
       "      <td>68.7357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      2010     2011     2012     2013     2014     2015     2016     2017  \\\n",
       "1  19.8284  26.9151  17.6709  49.6368  42.1120  40.7898  19.8859  30.8102   \n",
       "2  18.2749  29.1063  15.4340  46.9072  34.6492  39.3762  19.8985  30.1875   \n",
       "3  19.2606  34.9232  16.4762  47.0473  33.1698  41.5076  21.0710  34.0446   \n",
       "4  19.2495  40.0370  16.8430  41.6460  32.4070  38.7899  21.9256  35.2869   \n",
       "5  20.8877  44.3534  18.6354  36.9533  28.8598  34.5794  23.0014  41.0665   \n",
       "\n",
       "      2018     2019     2020  \n",
       "1  53.5765  36.3095  62.1936  \n",
       "2  55.2437  31.3643  50.6711  \n",
       "3  63.4528  33.6213  53.5768  \n",
       "4  70.1811  38.8115  60.2274  \n",
       "5  77.8090  45.2235  68.7357  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_flu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.91</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.24</td>\n",
       "      <td>2.48</td>\n",
       "      <td>4.75</td>\n",
       "      <td>4.25</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.13</td>\n",
       "      <td>2.54</td>\n",
       "      <td>1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.03</td>\n",
       "      <td>2.25</td>\n",
       "      <td>3.58</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4.54</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.97</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.24</td>\n",
       "      <td>3.83</td>\n",
       "      <td>4.72</td>\n",
       "      <td>2.74</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.92</td>\n",
       "      <td>4.37</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.37</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.19</td>\n",
       "      <td>3.48</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1.46</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2010  2011  2012  2013  2014  2015  2016  2017  2018  2019  2020\n",
       "1  2.91  4.88  4.24  2.48  4.75  4.25  1.52  1.81  1.13  2.54  1.83\n",
       "2  2.03  2.25  3.58  2.31  4.54  1.24  1.12  1.97  1.57  1.55  1.34\n",
       "3  1.24  3.83  4.72  2.74  3.55  3.27  1.65  1.54  0.74  2.18  1.01\n",
       "4  3.92  4.37  4.30  2.37  4.58  1.25  0.98  2.35  2.18  1.17  1.65\n",
       "5  3.19  3.48  4.50  3.04  3.23  0.65  0.89  1.28  0.87  1.46  2.23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AU_flu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.7</td>\n",
       "      <td>22.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>10.6</td>\n",
       "      <td>39.4</td>\n",
       "      <td>72.1</td>\n",
       "      <td>53.1</td>\n",
       "      <td>49.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.9</td>\n",
       "      <td>17.2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>4.8</td>\n",
       "      <td>23.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.1</td>\n",
       "      <td>23.9</td>\n",
       "      <td>69.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>47.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>18.8</td>\n",
       "      <td>6.9</td>\n",
       "      <td>27.3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.0</td>\n",
       "      <td>59.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>42.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1</td>\n",
       "      <td>9.4</td>\n",
       "      <td>20.3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>20.7</td>\n",
       "      <td>12.5</td>\n",
       "      <td>43.6</td>\n",
       "      <td>15.3</td>\n",
       "      <td>40.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.7</td>\n",
       "      <td>7.2</td>\n",
       "      <td>21.1</td>\n",
       "      <td>8.4</td>\n",
       "      <td>42.1</td>\n",
       "      <td>22.6</td>\n",
       "      <td>27.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>35.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2010  2011  2012  2013  2014  2015  2016  2017  2018  2019  2020\n",
       "1   9.7  22.3   6.2   3.7  19.4   8.3  10.6  39.4  72.1  53.1  49.1\n",
       "2   6.9  17.2  11.3   4.8  23.1  10.0  12.1  23.9  69.0  33.6  47.8\n",
       "3   6.0  14.3  18.8   6.9  27.3  14.0  13.5  17.0  59.6  23.0  42.4\n",
       "4   5.1   9.4  20.3   7.8  37.0  18.4  20.7  12.5  43.6  15.3  40.9\n",
       "5   4.7   7.2  21.1   8.4  42.1  22.6  27.2   9.9  35.3  11.3  28.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KR_flu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인플루엔자 데이터 정규화\n",
    "\n",
    "US_flu_norm, US_flu_scaler = normalize_flu(US_flu)\n",
    "AU_flu_norm, AU_flu_scaler = normalize_flu(AU_flu)\n",
    "KR_flu_norm, KR_flu_scaler = normalize_flu(KR_flu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US, AU, KR Google trends\n",
    "\n",
    "US_trends = pd.read_csv('./data/US_trends.csv')\n",
    "AU_trends = pd.read_csv('./data/AU_trends.csv')\n",
    "KR_trends = pd.read_csv('./data/KR_trends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trends 데이터 정규화\n",
    "\n",
    "US_trends_norm, _ = normalize_trends(US_trends)\n",
    "AU_trends_norm, _ = normalize_trends(AU_trends)\n",
    "KR_trends_norm, _ = normalize_trends(KR_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag load\n",
    "\n",
    "US_trends_lag = pd.read_csv('./data/v2/US_trends_lag.csv', index_col=0)\n",
    "AU_trends_lag = pd.read_csv('./data/v2/AU_trends_lag.csv', index_col=0)\n",
    "\n",
    "KR_news_lag = pd.read_csv('./data/v2/KR_news_lag.csv', index_col=0)\n",
    "KR_sns_lag = pd.read_csv('./data/v2/KR_sns_lag.csv', index_col=0)\n",
    "KR_trends_lag = pd.read_csv('./data/v2/KR_trends_lag.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max lag load\n",
    "\n",
    "US_trends_max_lag = pd.read_csv('./data/v2/US_trends_max_lag.csv', index_col=0)\n",
    "AU_trends_max_lag = pd.read_csv('./data/v2/AU_trends_max_lag.csv', index_col=0)\n",
    "\n",
    "KR_news_max_lag = pd.read_csv('./data/v2/KR_news_max_lag.csv', index_col=0)\n",
    "KR_sns_max_lag = pd.read_csv('./data/v2/KR_sns_max_lag.csv', index_col=0)\n",
    "KR_trends_max_lag = pd.read_csv('./data/v2/KR_trends_max_lag.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance load\n",
    "\n",
    "US_perform = pd.read_csv('./data/v2/US_perform4.csv', index_col=0)\n",
    "AU_perform = pd.read_csv('./data/v2/AU_perform4.csv', index_col=0)\n",
    "KR_perform = pd.read_csv('./data/v2/KR_perform4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction load\n",
    "\n",
    "with open('./data/v2/US_predict4.pkl', 'rb') as f:\n",
    "    US_predict = pickle.load(f)\n",
    "with open('./data/v2/AU_predict4.pkl', 'rb') as f:\n",
    "    AU_predict = pickle.load(f)\n",
    "with open('./data/v2/KR_predict4.pkl', 'rb') as f:\n",
    "    KR_predict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform_by_num_words\n",
    "\n",
    "with open('./data/v2/perform_by_num_words.pkl', 'rb') as f:\n",
    "    perform_by_num_words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Attention, Dense, Concatenate, Average, TimeDistributed, LeakyReLU, Add, Bidirectional, Average\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Encoder_v2(object):\n",
    "    def __init__(self, mode='A', is_sg=False, **kwargs):\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        self.past_year_num = kwargs['past_year_num'] if 'past_year_num' in kwargs else 1\n",
    "        self.news_word_num = kwargs['news_word_num'] if 'news_word_num' in kwargs else 1\n",
    "        self.sns_word_num = kwargs['sns_word_num'] if 'sns_word_num' in kwargs else 1\n",
    "        self.trend_word_num = kwargs['trend_word_num'] if 'trend_word_num' in kwargs else 1\n",
    "        self.layer_size = kwargs['layer_size'] if 'layer_size' in kwargs else 128\n",
    "        self.is_sg = is_sg # single encoder\n",
    "        self.mode = mode # N: news , S: SNS , T: Trend , A: N+S+T\n",
    "        \n",
    "        \n",
    "        ENC_LSTM_UNIT = self.layer_size\n",
    "        DENSE_UNIT = self.layer_size\n",
    "        \n",
    "        print(ENC_LSTM_UNIT, DENSE_UNIT)\n",
    "        \n",
    "        # Encoder1\n",
    "        enc1_inputs = Input(shape=(self.look_back, self.past_year_num+1), name='enc1_inputs')\n",
    "        \n",
    "        enc1_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc1_lstm1')\n",
    "        enc1_outputs1, enc1_h1, enc1_c1 = enc1_lstm1(enc1_inputs)\n",
    "        \n",
    "        # Encoder2\n",
    "        # news\n",
    "        enc2_news_inputs = Input(shape=(self.look_back, self.news_word_num), name='enc2_news_inputs')\n",
    "        \n",
    "        enc2_news_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc2_news_lstm1')\n",
    "        enc2_news_outputs1, enc2_news_h1, enc2_news_c1 = enc2_news_lstm1(enc2_news_inputs)\n",
    "        \n",
    "        enc2_news_outputs = Concatenate(axis=-1, name='enc2_news_outputs_concat')([enc1_outputs1, enc2_news_outputs1])\n",
    "        \n",
    "        enc2_news_states = [\n",
    "            [Concatenate(name='enc2_news_h1')([enc1_h1, enc2_news_h1]), Concatenate(name='enc2_news_c1')([enc1_c1, enc2_news_c1])],\n",
    "        ]\n",
    "        \n",
    "        # sns\n",
    "        enc2_sns_inputs = Input(shape=(self.look_back, self.sns_word_num), name='enc2_sns_inputs')\n",
    "        \n",
    "        enc2_sns_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc2_sns_lstm1')\n",
    "        enc2_sns_outputs1, enc2_sns_h1, enc2_sns_c1 = enc2_sns_lstm1(enc2_sns_inputs)\n",
    "        \n",
    "        enc2_sns_outputs = Concatenate(axis=-1, name='enc2_sns_outputs_concat')([enc1_outputs1, enc2_sns_outputs1])\n",
    "        \n",
    "        enc2_sns_states = [\n",
    "            [Concatenate(name='enc2_sns_h1')([enc1_h1, enc2_sns_h1]), Concatenate(name='enc2_sns_c1')([enc1_c1, enc2_sns_c1])],\n",
    "        ]\n",
    "        \n",
    "        # trend\n",
    "        enc2_trend_inputs = Input(shape=(self.look_back, self.trend_word_num), name='enc2_trend_inputs')\n",
    "        \n",
    "        enc2_trend_lstm1 = LSTM(ENC_LSTM_UNIT, return_state=True, return_sequences=True, name='enc2_trend_lstm1')\n",
    "        enc2_trend_outputs1, enc2_trend_h1, enc2_trend_c1 = enc2_trend_lstm1(enc2_trend_inputs)\n",
    "        \n",
    "        enc2_trend_outputs = Concatenate(axis=-1, name='enc2_trend_outputs_concat')([enc1_outputs1, enc2_trend_outputs1])\n",
    "        \n",
    "        enc2_trend_states = [\n",
    "            [Concatenate(name='enc2_trend_h1')([enc1_h1, enc2_trend_h1]), Concatenate(name='enc2_trend_c1')([enc1_c1, enc2_trend_c1])],\n",
    "        ]\n",
    "        \n",
    "        # Decoder\n",
    "        if is_sg:\n",
    "            enc2_news_states = [enc2_news_h1, enc2_news_c1]\n",
    "            enc2_news_outputs = enc2_news_outputs1\n",
    "            enc2_sns_states = [enc2_sns_h1, enc2_sns_c1]\n",
    "            enc2_sns_outputs = enc2_sns_outputs1\n",
    "            enc2_trend_states = [enc2_trend_h1, enc2_trend_c1]\n",
    "            enc2_trend_outputs = enc2_trend_outputs1\n",
    "            dec_lstm_unit = ENC_LSTM_UNIT\n",
    "            \n",
    "        else:\n",
    "            enc2_news_states = enc2_news_states[0]\n",
    "            enc2_sns_states = enc2_sns_states[0]\n",
    "            enc2_trend_states = enc2_trend_states[0]\n",
    "            dec_lstm_unit = ENC_LSTM_UNIT * 2\n",
    "        \n",
    "        \n",
    "        # news\n",
    "        dec_news_fn = self.news_word_num if is_sg else self.news_word_num+1\n",
    "        dec_news_inputs = Input(shape=(None, dec_news_fn), name='dec_news_inputs')\n",
    "        \n",
    "        dec_news_lstm1 = LSTM(dec_lstm_unit, return_state=True, return_sequences=True, name='dec_news_lstm1')\n",
    "        dec_news_outputs1, _, _ = dec_news_lstm1(dec_news_inputs, initial_state=enc2_news_states)\n",
    "        \n",
    "        dec_news_attn = Attention(name='dec_news_attn')([dec_news_outputs1, enc2_news_outputs])\n",
    "        \n",
    "        # sns\n",
    "        dec_sns_fn = self.sns_word_num if is_sg else self.sns_word_num+1\n",
    "        dec_sns_inputs = Input(shape=(None, dec_sns_fn), name='dec_sns_inputs')\n",
    "        \n",
    "        dec_sns_lstm1 = LSTM(dec_lstm_unit, return_state=True, return_sequences=True, name='dec_sns_lstm1')\n",
    "        dec_sns_outputs1, _, _ = dec_sns_lstm1(dec_sns_inputs, initial_state=enc2_sns_states)\n",
    "        \n",
    "        dec_sns_attn = Attention(name='dec_sns_attn')([dec_sns_outputs1, enc2_sns_outputs])\n",
    "        \n",
    "        # trend\n",
    "        dec_trend_fn = self.trend_word_num if is_sg else self.trend_word_num+1\n",
    "        dec_trend_inputs = Input(shape=(None, dec_trend_fn), name='dec_trend_inputs')\n",
    "        \n",
    "        dec_trend_lstm1 = LSTM(dec_lstm_unit, return_state=True, return_sequences=True, name='dec_trend_lstm1')\n",
    "        dec_trend_outputs1, _, _ = dec_trend_lstm1(dec_trend_inputs, initial_state=enc2_trend_states)\n",
    "        \n",
    "        dec_trend_attn = Attention(name='dec_trend_attn')([dec_trend_outputs1, enc2_trend_outputs])\n",
    "        \n",
    "        # concat & dense\n",
    "        if mode == 'N':\n",
    "            enc2_inputs = [enc2_news_inputs]\n",
    "            enc2_outputs = enc2_news_outputs\n",
    "            enc2_states = enc2_news_states\n",
    "            dec_inputs = [dec_news_inputs]\n",
    "            dec_attn_concat = dec_news_attn\n",
    "            dec_lstm1 = dec_news_lstm1\n",
    "        elif mode == 'S':\n",
    "            enc2_inputs = [enc2_sns_inputs]\n",
    "            enc2_outputs = enc2_sns_outputs\n",
    "            enc2_states = enc2_sns_states\n",
    "            dec_inputs = [dec_sns_inputs]\n",
    "            dec_attn_concat = dec_sns_attn\n",
    "            dec_lstm1 = dec_sns_lstm1\n",
    "        elif mode == 'T':\n",
    "            enc2_inputs = [enc2_trend_inputs]\n",
    "            enc2_outputs = enc2_trend_outputs\n",
    "            enc2_states = enc2_trend_states\n",
    "            dec_inputs = [dec_trend_inputs]\n",
    "            dec_attn_concat = dec_trend_attn\n",
    "            dec_lstm1 = dec_trend_lstm1\n",
    "        elif mode == 'NS':\n",
    "            enc2_inputs = [enc2_news_inputs, enc2_sns_inputs]\n",
    "            dec_inputs = [dec_news_inputs, dec_sns_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_news_attn, dec_sns_attn])\n",
    "        elif mode == 'ST':\n",
    "            enc2_inputs = [enc2_sns_inputs, enc2_trend_inputs]\n",
    "            dec_inputs = [dec_sns_inputs, dec_trend_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_sns_attn, dec_trend_attn])\n",
    "        elif mode == 'TN':\n",
    "            enc2_inputs = [enc2_trend_inputs, enc2_news_inputs]\n",
    "            dec_inputs = [dec_trend_inputs, dec_news_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_trend_attn, dec_news_attn])\n",
    "        else:\n",
    "            enc2_inputs = [enc2_news_inputs, enc2_sns_inputs, enc2_trend_inputs]\n",
    "            enc2_outputs = [enc2_news_outputs, enc2_sns_outputs, enc2_trend_outputs]\n",
    "            enc2_states = [enc2_news_states, enc2_sns_states, enc2_trend_states]\n",
    "            dec_inputs = [dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "            dec_attn_concat = Concatenate(name='decattn_concat')([dec_news_attn, dec_sns_attn, dec_trend_attn])\n",
    "            \n",
    "            \n",
    "        dec_dense1 = TimeDistributed(Dense(DENSE_UNIT, name='dec_dense1'), name='dec_time1')\n",
    "        dec_dense2 = TimeDistributed(Dense(1, name='dec_dense2'), name='dec_time2')\n",
    "        leaky_relu = LeakyReLU(alpha=0.1)\n",
    "        dec_ouputs = dec_dense1(dec_attn_concat)\n",
    "        dec_ouputs = dec_dense2(dec_ouputs)\n",
    "        dec_ouputs = leaky_relu(dec_ouputs)\n",
    "        \n",
    "        if is_sg:\n",
    "            model = Model([enc2_inputs, dec_inputs], dec_ouputs)\n",
    "        else:\n",
    "            model = Model([enc1_inputs, enc2_inputs, dec_inputs], dec_ouputs)\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        self.model = model\n",
    "        \n",
    "        if mode == 'A':\n",
    "            dec_news_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_news_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_news_c_input')\n",
    "            ]\n",
    "            dec_sns_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_sns_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_sns_c_input')\n",
    "            ]\n",
    "            dec_trend_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_trend_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_trend_c_input')\n",
    "            ]\n",
    "            \n",
    "            dec_news_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_news_hidden_input')\n",
    "            dec_sns_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_sns_hidden_input')\n",
    "            dec_trend_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_trend_hidden_input')\n",
    "            \n",
    "            dec_news_output2, dec_news_state_h, dec_news_state_c = dec_news_lstm1(\n",
    "                dec_news_inputs, initial_state=dec_news_state_inputs\n",
    "            )\n",
    "            dec_sns_output2, dec_sns_state_h, dec_sns_state_c = dec_sns_lstm1(\n",
    "                dec_sns_inputs, initial_state=dec_sns_state_inputs\n",
    "            )\n",
    "            dec_trend_output2, dec_trend_state_h, dec_trend_state_c = dec_trend_lstm1(\n",
    "                dec_trend_inputs, initial_state=dec_trend_state_inputs\n",
    "            )\n",
    "            dec_news_attn2 = Attention(name='dec_news_attn2')([dec_news_output2, dec_news_hidden_inputs])\n",
    "            dec_sns_attn2 = Attention(name='dec_sns_attn2')([dec_sns_output2, dec_sns_hidden_inputs])\n",
    "            dec_trend_attn2 = Attention(name='dec_trend_attn2')([dec_trend_output2, dec_trend_hidden_inputs])\n",
    "            \n",
    "            dec_attn_concat2 = Concatenate(name='dec_attn_concat2')([\n",
    "                dec_news_attn2, dec_sns_attn2, dec_trend_attn2\n",
    "            ])\n",
    "            dec_outputs2 = dec_dense1(dec_attn_concat2)\n",
    "            dec_outputs2 = dec_dense2(dec_outputs2)\n",
    "            dec_outputs2 = leaky_relu(dec_outputs2)\n",
    "            \n",
    "            dec_inputs = [dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "            dec_hidden_inputs = [dec_news_hidden_inputs, dec_sns_hidden_inputs, dec_trend_hidden_inputs]\n",
    "            \n",
    "            self.decoder_model = Model([dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "                                      + [dec_news_hidden_inputs, dec_sns_hidden_inputs, dec_trend_hidden_inputs]\n",
    "                                      + [dec_news_state_inputs, dec_sns_state_inputs, dec_trend_state_inputs],\n",
    "                                      [dec_outputs2] \n",
    "                                       + [[dec_news_state_h, dec_news_state_c],\n",
    "                                          [dec_sns_state_h, dec_sns_state_c],\n",
    "                                          [dec_trend_state_h, dec_trend_state_c]])\n",
    "        else:\n",
    "            dec_state_inputs = [\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_h_input'),\n",
    "                Input(shape=(dec_lstm_unit,), name='dec_c_input')\n",
    "            ]\n",
    "\n",
    "            dec_hidden_inputs = Input(shape=(self.look_back, dec_lstm_unit), name='dec_hidden_input')\n",
    "\n",
    "            dec_outputs2, dec_state_h, dec_state_c = dec_lstm1(dec_inputs[0],\n",
    "                                                                initial_state=dec_state_inputs)\n",
    "            dec_attn2 = Attention(name='dec_test_attn2')([dec_outputs2, dec_hidden_inputs])\n",
    "            dec_outputs2 = dec_dense1(dec_attn2)\n",
    "            dec_outputs2 = dec_dense2(dec_outputs2)\n",
    "            dec_outputs2 = leaky_relu(dec_outputs2)\n",
    "            \n",
    "            self.decoder_model = Model([dec_inputs] + [dec_hidden_inputs] + dec_state_inputs,\n",
    "                                   [dec_outputs2] + [dec_state_h, dec_state_c])\n",
    "        \n",
    "        if is_sg:\n",
    "          self.encoder_model = Model([enc2_inputs], [enc2_outputs] + enc2_states)\n",
    "        else:\n",
    "          self.encoder_model = Model([enc1_inputs, enc2_inputs], [enc2_outputs] + enc2_states)\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        if not self.is_sg:\n",
    "          self.encoder_model.get_layer('enc1_lstm1').set_weights(\n",
    "              self.model.get_layer('enc1_lstm1').get_weights())\n",
    "        self.decoder_model.get_layer('dec_time1').set_weights(\n",
    "            self.model.get_layer('dec_time1').get_weights())\n",
    "        self.decoder_model.get_layer('dec_time2').set_weights(\n",
    "            self.model.get_layer('dec_time2').get_weights())\n",
    "        \n",
    "        if self.mode == 'N':\n",
    "            web_types = ['news']\n",
    "        elif self.mode == 'S':\n",
    "            web_types = ['sns']\n",
    "        elif self.mode == 'T':\n",
    "            web_types = ['trend']\n",
    "        else:\n",
    "            web_types = ['news', 'sns', 'trend']\n",
    "        \n",
    "        for target_name in web_types:\n",
    "            self.encoder_model.get_layer('enc2_' + target_name + '_lstm1').set_weights(\n",
    "                self.model.get_layer('enc2_' + target_name + '_lstm1').get_weights())\n",
    "            self.decoder_model.get_layer('dec_' + target_name + '_lstm1').set_weights(\n",
    "                self.model.get_layer('dec_' + target_name + '_lstm1').get_weights())\n",
    "            \n",
    "        \n",
    "    def transform_data(self, truth, news_data, sns_data, trends_data, \n",
    "                       news_lag, sns_lag, trends_lag,\n",
    "                       year_week, look_back, look_ahead):\n",
    "        \n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _encoder1(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                tmp1 = truth.loc[week-look_back:week-1,\n",
    "                                               [str(y) for y in range(2011, 2017)]]\n",
    "                tmp2 = truth.loc[week-look_back:week-1, '20'+year]\n",
    "                encoder1_input = pd.concat((tmp1, tmp2), axis=1)\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week,\n",
    "                                     [str(y) for y in range(2010, 2016)]]\n",
    "                tmp2 = truth.loc[:week-1,\n",
    "                                     [str(y) for y in range(2011, 2017)]]\n",
    "                tmp2.columns = [str(y) for y in range(2010, 2016)]\n",
    "                tmp2 = pd.concat((tmp1, tmp2))\n",
    "\n",
    "                tmp3 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp4 = truth.loc[:week-1, '20'+year]\n",
    "                tmp4.columns = [['20'+str(int(year)-1)]]\n",
    "                tmp4 = pd.concat((tmp3, tmp4))\n",
    "                encoder1_input = pd.concat((tmp2, tmp4), axis=1)\n",
    "            return encoder1_input.values\n",
    "        \n",
    "        def _encoder2(year_week, look_back):\n",
    "            if self.mode == 'N':\n",
    "                web_data = [news_data]\n",
    "                web_lag = [news_lag]\n",
    "            elif self.mode == 'S':\n",
    "                web_data = [sns_data]\n",
    "                web_lag = [sns_lag]\n",
    "            elif self.mode == 'T':\n",
    "                web_data = [trends_data]\n",
    "                web_lag = [trends_lag]\n",
    "            else:\n",
    "                web_data = [news_data, sns_data, trends_data]\n",
    "                web_lag = [news_lag, sns_lag, trends_lag]\n",
    "            \n",
    "\n",
    "            encoder2_input = []\n",
    "            for data, lag_df in zip(web_data, web_lag):\n",
    "                tmp = []\n",
    "                for w in lag_df.columns:\n",
    "                    lag = int(lag_df[w]['max_lag'])\n",
    "                    origin_point = data[data['weeks'] == year_week].index[0]\n",
    "                    start_point = origin_point-lag-look_back\n",
    "                    end_point = start_point+look_back-1\n",
    "\n",
    "                    tmp.append(\n",
    "                            data.loc[start_point:end_point, w].apply(\n",
    "                            lambda x: np.random.uniform(1e-6, 1e-4) if x == 0 else x).tolist())\n",
    "                tmp = np.array(tmp).T\n",
    "                encoder2_input.append(tmp)\n",
    "            if len(encoder2_input) == 1:\n",
    "                encoder2_input = tmp\n",
    "                \n",
    "            return encoder2_input\n",
    "        \n",
    "        def _decoder(year_week, look_ahead):\n",
    "            year, week = year_week.split('-')\n",
    "            max_week = 52\n",
    "            week = int(week)\n",
    "            if week > 1:\n",
    "                week = week - 1\n",
    "            else:\n",
    "                week = 52\n",
    "                year = str(int(year) - 1)\n",
    "\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "\n",
    "            ground_truth = ground_truth.values.reshape(-1, 1)\n",
    "            \n",
    "            if self.mode == 'N':\n",
    "                web_data = [news_data]\n",
    "                web_lag = [news_lag]\n",
    "            elif self.mode == 'S':\n",
    "                web_data = [sns_data]\n",
    "                web_lag = [sns_lag]\n",
    "            elif self.mode == 'T':\n",
    "                web_data = [trends_data]\n",
    "                web_lag = [trends_lag]\n",
    "            else:\n",
    "                web_data = [news_data, sns_data, trends_data]\n",
    "                web_lag = [news_lag, sns_lag, trends_lag]\n",
    "            \n",
    "            decoder_input = []\n",
    "            \n",
    "            for data, lag_df in zip(web_data, web_lag):\n",
    "                tmp = []\n",
    "                for i, w in enumerate(list(lag_df.columns)):\n",
    "                    lag = int(lag_df[w]['max_lag'])\n",
    "                    origin_point = data[data['weeks'] == year_week].index[0]\n",
    "                    start_point = origin_point-lag\n",
    "                    end_point = start_point+look_ahead-1 if lag >= look_ahead else origin_point\n",
    "\n",
    "                    target_list = data.loc[start_point:end_point, w].tolist()\n",
    "                    random_pad = np.random.uniform(1e-6, 1e-4, (look_ahead,))\n",
    "                    for idx in range(len(target_list)):\n",
    "                        if target_list[idx] == 0:\n",
    "                            continue\n",
    "                        random_pad[idx] = target_list[idx]\n",
    "\n",
    "                    tmp.append(random_pad)\n",
    "                tmp = np.array(tmp).T\n",
    "                tmp = np.concatenate([tmp, ground_truth], axis=-1)\n",
    "                decoder_input.append(tmp)\n",
    "            if len(decoder_input) == 1:\n",
    "                decoder_input = tmp\n",
    "            \n",
    "            return decoder_input\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        encoder1_input = _encoder1(year, week, look_back)\n",
    "        encoder2_input = _encoder2(year_week, look_back)\n",
    "        decoder_input = _decoder(year_week, look_ahead)      \n",
    "\n",
    "        return encoder1_input, encoder2_input, decoder_input, ground_truth    \n",
    "                \n",
    "        \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      news_data=None, sns_data=None, trends_data=None,\n",
    "                      news_lag=None, sns_lag=None, trends_lag=None,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        enc1_inputs = []\n",
    "        enc2_inputs = []\n",
    "        dec_inputs = []\n",
    "        ground_truth = []\n",
    "        \n",
    "        enc2_news_inputs = []\n",
    "        enc2_sns_inputs = []\n",
    "        enc2_trend_inputs = []\n",
    "        dec_news_inputs = []\n",
    "        dec_sns_inputs = []\n",
    "        dec_trend_inputs = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            e1, e2, d, gt = self.transform_data(truth, \n",
    "                                                news_data, sns_data, trends_data,\n",
    "                                                news_lag, sns_lag, trends_lag,\n",
    "                                                aw, look_back, look_ahead)\n",
    "            \n",
    "            enc1_inputs.append(e1)\n",
    "            ground_truth.append(gt)\n",
    "            \n",
    "            if self.mode == 'A':\n",
    "                enc2_news_inputs.append(e2[0])\n",
    "                enc2_sns_inputs.append(e2[1])\n",
    "                enc2_trend_inputs.append(e2[2])\n",
    "                dec_news_inputs.append(d[0])\n",
    "                dec_sns_inputs.append(d[1])\n",
    "                dec_trend_inputs.append(d[2])\n",
    "            else:\n",
    "                enc2_inputs.append(e2)\n",
    "                dec_inputs.append(d)\n",
    "            \n",
    "            \n",
    "        if self.mode == 'A':\n",
    "            x = [enc1_inputs,\n",
    "                 enc2_news_inputs, enc2_sns_inputs, enc2_trend_inputs,\n",
    "                 dec_news_inputs, dec_sns_inputs, dec_trend_inputs]\n",
    "        else:\n",
    "          if self.is_sg:\n",
    "            x = [np.concatenate([np.array(enc2_inputs),\n",
    "                                 np.array(enc1_inputs)[:, :, -1:]], axis=-1),\n",
    "                 dec_inputs]\n",
    "          else:\n",
    "            enc1_inputs = np.array(enc1_inputs)[:, :, -1-self.past_year_num:]\n",
    "            x = [enc1_inputs, enc2_inputs, dec_inputs]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_LSTM():\n",
    "    def __init__(self, **kwargs):\n",
    "        LSTM_UNIT = 64\n",
    "        DENSE_UNIT = 64\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        \n",
    "        inputs = Input(shape=(self.look_back, 1), name='input1')\n",
    "        lstm1 = LSTM(LSTM_UNIT, return_state=True, name='lstm1')\n",
    "        dense1 = Dense(DENSE_UNIT, name='dense1')\n",
    "        dense2 = Dense(1)\n",
    "        leaky_relu = LeakyReLU(alpha=0.1)\n",
    "        \n",
    "        outputs, _, _ = lstm1(inputs)\n",
    "        outputs = dense1(outputs)\n",
    "        outputs = dense2(outputs)\n",
    "        outputs = leaky_relu(outputs)\n",
    "        \n",
    "        model = Model([inputs], outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "    def transform_data(self, truth, year_week, look_back, look_ahead):\n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _inputs(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                inputs = truth.loc[week-look_back:week-1, '20'+year]\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp2 = truth.loc[:week-1, '20'+year]\n",
    "                tmp2.columns = [['20'+str(int(year)-1)]]\n",
    "                inputs = pd.concat((tmp1, tmp2))\n",
    "            return inputs.values.reshape(-1, 1)\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        inputs = _inputs(year, week, look_back)\n",
    "        \n",
    "        return inputs, ground_truth\n",
    "    \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        ground_truth = []\n",
    "        inputs = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            i, gt = self.transform_data(truth, aw, look_back, look_ahead)\n",
    "            inputs.append(i)\n",
    "            ground_truth.append(gt)\n",
    "        \n",
    "        x = [inputs]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEFSI():\n",
    "    def __init__(self, **kwargs):\n",
    "        LSTM_UNIT = 64\n",
    "        DENSE_UNIT = 64\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        \n",
    "        x1 = Input(shape=(self.look_back, 1), name='x1')\n",
    "        left_lstm1 = LSTM(LSTM_UNIT, return_sequences=True, return_state=True, name='left_lstm1')\n",
    "        left_lstm2 = LSTM(LSTM_UNIT, return_state=True, name='left_lstm2')\n",
    "        left_dense1 = Dense(DENSE_UNIT, name='left_dense1')\n",
    "        \n",
    "        left_outputs, _, _ = left_lstm1(x1)\n",
    "        left_outputs, _, _ = left_lstm2(left_outputs)\n",
    "        left_outputs = left_dense1(left_outputs)\n",
    "        \n",
    "        x2 = Input(shape=(self.look_back, 1), name='x2')\n",
    "        right_lstm1 = LSTM(LSTM_UNIT, return_state=True, name='right_lsmt1')\n",
    "        right_dense1 = Dense(DENSE_UNIT, )\n",
    "        \n",
    "        right_outputs, _, _ = right_lstm1(x2)\n",
    "        right_outputs = right_dense1(right_outputs)\n",
    "        \n",
    "        merge = Average(name='merge')([left_outputs, right_outputs])\n",
    "        merge_dense = Dense(1, name='merge_dense')\n",
    "        outputs = merge_dense(merge)\n",
    "        \n",
    "        model = Model([x1, x2], outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "    def transform_data(self, truth, year_week, look_back, look_ahead):\n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _inputs(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                x1 = truth.loc[week-look_back:week-1, '20'+year]\n",
    "                x2 = truth.loc[week-look_back:week-1, '20'+str(int(year)-1)]\n",
    "                inputs = [x1.values.reshape(-1, 1), x2.values.reshape(-1, 1)]\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp2 = truth.loc[:week-1, '20'+year]\n",
    "                tmp2.columns = [['20'+str(int(year)-1)]]\n",
    "                x1 = pd.concat((tmp1, tmp2)).values\n",
    "\n",
    "                tmp3 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-2)]\n",
    "                tmp4 = truth.loc[:week-1, '20'+str(int(year)-1)]\n",
    "                tmp4.columns = [['20'+str(int(year)-2)]]\n",
    "                x2 = pd.concat((tmp3, tmp4)).values\n",
    "                inputs = [x1.reshape(-1, 1), x2.reshape(-1, 1)]\n",
    "            return inputs\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        inputs = _inputs(year, week, look_back)\n",
    "        \n",
    "        return inputs, ground_truth\n",
    "    \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        ground_truth = []\n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            i, gt = self.transform_data(truth, aw, look_back, look_ahead)\n",
    "            x1.append(i[0])\n",
    "            x2.append(i[1])\n",
    "            ground_truth.append(gt)\n",
    "        \n",
    "        x = [x1, x2]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq():\n",
    "    def __init__(self, **kwargs):\n",
    "        ENC_LSTM_UNIT = 64\n",
    "        DEC_LSTM_UNIT = 128\n",
    "        DROPOUT = 0.2\n",
    "        \n",
    "        self.look_back = kwargs['look_back']\n",
    "        self.look_ahead = kwargs['look_ahead']\n",
    "        \n",
    "        enc_inputs = Input(shape=(self.look_back, 1), name='enc_input')\n",
    "        enc_lstm1 = Bidirectional(LSTM(ENC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                                       dropout=DROPOUT, name='enc_lstm1'), name='enc_bilstm1')\n",
    "        enc_lstm2 = Bidirectional(LSTM(ENC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                                       dropout=DROPOUT, name='enc_lstm2'), name='enc_bilstm2')\n",
    "        enc_lstm3 = Bidirectional(LSTM(ENC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                                       dropout=DROPOUT, name='enc_lstm3'), name='enc_bilstm3')\n",
    "        enc_outputs, _, _, _, _ = enc_lstm1(enc_inputs)\n",
    "        enc_outputs, _, _, _, _ = enc_lstm2(enc_outputs)\n",
    "        enc_outputs, fh, fc, bh, bc = enc_lstm3(enc_outputs)\n",
    "        \n",
    "        state_h = Concatenate(name='enc_h_concat')([fh, bh])\n",
    "        state_c = Concatenate(name='enc_c_concat')([fc, bc])\n",
    "        \n",
    "        dec_inputs = Input(shape=(None, 1), name='dec_input')\n",
    "        dec_lstm1 = LSTM(DEC_LSTM_UNIT, return_sequences=True, return_state=True,\n",
    "                         dropout=DROPOUT, name='dec_lstm1')\n",
    "        dec_outputs, _, _ = dec_lstm1(dec_inputs, initial_state=[state_h, state_c])\n",
    "        \n",
    "        dec_attn = Attention(name='dec_attn')([dec_outputs, enc_outputs])\n",
    "        dec_concat = Concatenate(name='dec_concat')([dec_outputs, dec_attn])\n",
    "        dec_dense1 = TimeDistributed(Dense(1, name='dense1'), name='time1')\n",
    "        outputs = dec_dense1(dec_concat)\n",
    "        \n",
    "        model = Model([enc_inputs, dec_inputs], outputs)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        dec_state_inputs = [\n",
    "            Input(shape=(DEC_LSTM_UNIT,), name='dec_h_input'),\n",
    "            Input(shape=(DEC_LSTM_UNIT,), name='dec_c_input')\n",
    "        ]\n",
    "        dec_hidden_inputs = Input(shape=(self.look_back, DEC_LSTM_UNIT), name='dec_hidden_input')\n",
    "        \n",
    "        dec_outputs2, dec_state_h, dec_state_c = dec_lstm1(dec_inputs, \n",
    "                                                          initial_state=dec_state_inputs)\n",
    "        dec_attn2 = Attention(name='dec_test_attn2')([dec_outputs2, dec_hidden_inputs])\n",
    "        dec_concat2 = Concatenate(name='dec_test_concat2')([dec_outputs2, dec_attn2])\n",
    "        dec_outputs2 = dec_dense1(dec_concat2)\n",
    "        \n",
    "        self.encoder_model = Model(enc_inputs, [enc_outputs, state_h, state_c])\n",
    "        self.decoder_model = Model([dec_inputs] + [dec_hidden_inputs] + dec_state_inputs,\n",
    "                                   [dec_outputs2] + [dec_state_h, dec_state_c])\n",
    "        \n",
    "    def fit(self, x, y, val_x, val_y, epochs=10, batch_size=32, callbacks=None, verbose=1):\n",
    "        history = self.model.fit(x, y, epochs=epochs, batch_size=batch_size,\n",
    "                                validation_data=(val_x, val_y), callbacks=callbacks,\n",
    "                                verbose=verbose)\n",
    "        return history\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        self.encoder_model.get_layer('enc_bilstm1').set_weights(\n",
    "            self.model.get_layer('enc_bilstm1').get_weights())\n",
    "        self.encoder_model.get_layer('enc_bilstm2').set_weights(\n",
    "            self.model.get_layer('enc_bilstm2').get_weights())\n",
    "        self.encoder_model.get_layer('enc_bilstm3').set_weights(\n",
    "            self.model.get_layer('enc_bilstm3').get_weights())\n",
    "        self.decoder_model.get_layer('dec_lstm1').set_weights(\n",
    "            self.model.get_layer('dec_lstm1').get_weights())\n",
    "        self.decoder_model.get_layer('time1').set_weights(\n",
    "            self.model.get_layer('time1').get_weights())  \n",
    "        \n",
    "    def transform_data(self, truth, year_week, look_back, look_ahead):\n",
    "        def _ground_truth(week, year, look_ahead):\n",
    "            week = int(week)\n",
    "            max_week = 52\n",
    "            if week+look_ahead-1 > max_week:\n",
    "                tmp1 = truth.loc[week:max_week, '20'+year]\n",
    "                tmp2 = truth.loc[1:look_ahead-max_week+week-1, '20'+str(int(year)+1)]\n",
    "                ground_truth = pd.concat((tmp1, tmp2))\n",
    "            else:\n",
    "                ground_truth = truth.loc[week:week+look_ahead-1, '20'+year]\n",
    "            return ground_truth.values.reshape(-1, 1)\n",
    "        \n",
    "        def _inputs(year, week, look_back):\n",
    "            week = int(week)\n",
    "            if week > look_back:\n",
    "                inputs = truth.loc[week-look_back:week-1, '20'+year]\n",
    "            else:\n",
    "                max_week = 52\n",
    "                tmp1 = truth.loc[max_week-look_back+week:max_week, '20'+str(int(year)-1)]\n",
    "                tmp2 = truth.loc[:week-1, '20'+year]\n",
    "                tmp2.columns = [['20'+str(int(year)-1)]]\n",
    "                inputs = pd.concat((tmp1, tmp2))\n",
    "            return inputs.values.reshape(-1, 1)\n",
    "        \n",
    "        year, week = year_week.split('-')\n",
    "        ground_truth = _ground_truth(week, year, look_ahead)\n",
    "        inputs = _inputs(year, week, look_back)\n",
    "        inputs = [inputs, np.concatenate([inputs[-1, :].reshape(-1, 1), ground_truth[:-1, :]])]\n",
    "        \n",
    "        return inputs, ground_truth        \n",
    "    \n",
    "    def generate_data(self, truth, available_weeks, look_back, look_ahead,\n",
    "                      val_split_size=0.8, test_split_size=0.8):\n",
    "        \n",
    "        test_size = int(len(available_weeks) * test_split_size)\n",
    "        val_size = int(test_size * val_split_size)\n",
    "        \n",
    "        ground_truth = []\n",
    "        enc_inputs = []\n",
    "        dec_inputs = []\n",
    "        \n",
    "        for aw in available_weeks:\n",
    "            i, gt = self.transform_data(truth, aw, look_back, look_ahead)\n",
    "            enc_inputs.append(i[0])\n",
    "            dec_inputs.append(i[1])\n",
    "            ground_truth.append(gt)\n",
    "        \n",
    "        x = [enc_inputs, dec_inputs]\n",
    "        \n",
    "        x_total = [np.array(e) for e in x]\n",
    "        x_train = [np.array(e[:val_size]) for e in x]\n",
    "        x_val = [np.array(e[val_size:test_size]) for e in x]\n",
    "        x_test = [np.array(e[test_size:]) for e in x]\n",
    "\n",
    "        y = np.array(ground_truth)\n",
    "        y_train = y[:val_size]\n",
    "        y_val = y[val_size:test_size]\n",
    "        y_test = y[test_size:]\n",
    "\n",
    "        return [x_train, x_val, x_test, x_total], [y_train, y_val, y_test], val_size, test_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter\n",
    "\n",
    "look_back = 8\n",
    "look_ahead = 10\n",
    "past_year_num = 6\n",
    "\n",
    "max_lag = 14\n",
    "collect_err = 22\n",
    "\n",
    "test_split_size = 0.65\n",
    "val_split_size = 0.8\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_by_num_words = {\n",
    "    'US': {},\n",
    "    'AU': {},\n",
    "    'KR': {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# US_trends MEDIF 적용\n",
    "epochs = 100\n",
    "is_sg = False\n",
    "sg_path = '_2y' #if is_sg else ''\n",
    "\n",
    "US_perform_by_model = {}\n",
    "US_prd_by_model = {}\n",
    "\n",
    "for i in [1]:\n",
    "#     perform_by_num_words['US'][i] = {}\n",
    "    mode = 'T'\n",
    "\n",
    "    mode_name = 'ME-' + mode + '(US)'\n",
    "    mode_name = mode_name+'(2Y)' #if is_sg else mode_name\n",
    "    \n",
    "    US_perform_by_model[mode_name] = {}\n",
    "    print('mode:', mode_name)\n",
    "    clear_session()\n",
    "    \n",
    "    target_max_lag = US_trends_max_lag.T.sort_values(by='coef', ascending=False).head(i).T\n",
    "    print(i, target_max_lag.columns)\n",
    "    trend_word_num = len(target_max_lag.columns)\n",
    "    trend_word_num = trend_word_num+1 if is_sg else trend_word_num\n",
    "\n",
    "    me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                          past_year_num=0,\n",
    "                          trend_word_num=trend_word_num,\n",
    "                          mode=mode,\n",
    "                          is_sg=is_sg)\n",
    "\n",
    "    available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "    x, y, val_size, test_size = me.generate_data(CDC_norm, available_weeks,\n",
    "                                                 look_back, look_ahead,\n",
    "                                                 test_split_size=test_split_size,\n",
    "                                                 val_split_size=val_split_size,\n",
    "                                                 trends_data=US_trends_norm,\n",
    "                                                 trends_lag=target_max_lag)\n",
    "\n",
    "    x_train, x_val, x_test, x_total = x\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "    cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "    history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                      callbacks=[cp], verbose=0)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    min_epoch = 90\n",
    "    target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                    min(history.history['val_loss'][min_epoch:]))\n",
    "    best_path = './models/v2/US_me_' + mode + '_lb' + str(look_back) + sg_path + '_best.h5'\n",
    "    print(target_epoch, best_path)\n",
    "    shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "    test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=0,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode, \n",
    "                              is_sg=is_sg)\n",
    "    test_me.load_model(best_path)\n",
    "\n",
    "    start_week = int(available_weeks[0].split('-')[1])\n",
    "    end_week = int(available_weeks[-1].split('-')[1])\n",
    "    target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                    + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "    prd_total = []\n",
    "\n",
    "    for target_idx in range(90-1):\n",
    "        if is_sg:\n",
    "          enc_inputs = [x_total[0][target_idx:target_idx+1]]\n",
    "          dec_input = x_total[1][target_idx, 0, :].reshape(1, 1, x_total[1].shape[2])\n",
    "        else:\n",
    "          enc_inputs = [x_total[0][target_idx:target_idx+1],\n",
    "                        x_total[1][target_idx:target_idx+1]]\n",
    "          dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "          \n",
    "        enc_out, enc_h, enc_c = test_me.encoder_model.predict(enc_inputs)\n",
    "\n",
    "        tmp = []\n",
    "        for ahead in range(look_ahead):\n",
    "            prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "            prd_val = prd[0, 0, 0]\n",
    "            if prd_val > 0:\n",
    "                tmp.append([prd_val])\n",
    "            else:\n",
    "                tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "            if ahead < look_ahead - 1:\n",
    "              if is_sg:\n",
    "                dec_input = x_total[1][target_idx, ahead+1, :].reshape(1, 1, x_total[1].shape[2])\n",
    "              else:\n",
    "                dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "              dec_input[0, 0, -1] = prd_val\n",
    "            enc_h, enc_c = dec_h, dec_c\n",
    "        prd_total.append(tmp)\n",
    "    prd_total = np.array(prd_total)\n",
    "    US_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "    show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "    prd_train = prd_total[:val_size]\n",
    "    prd_val = prd_total[val_size:test_size]\n",
    "    prd_test = prd_total[test_size:]\n",
    "\n",
    "    inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "    inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                           inverse_prd_total[:val_size], look_ahead)\n",
    "    rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                     inverse_prd_total[val_size:test_size], look_ahead)\n",
    "    rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                        inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "    for ahead in range(look_ahead):\n",
    "        ah = str(ahead+1)\n",
    "        US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "        US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "        US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "        US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "        US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "        US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "        US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "        US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "        US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "        US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "        US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "        US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    perform_by_num_words['US'][i]['perform'][mode_name] = US_perform_by_model[mode_name].copy()\n",
    "    perform_by_num_words['US'][i]['prd'][mode_name] = prd_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([US_perform, pd.DataFrame(US_perform_by_model).T])[\n",
    "  ['total_%s_%s' % (ev, i) for i in [1, 2, 3] for ev in ['rmse', 'mape', 'corr']]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic_LSTM\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# US_perform_by_model = {}\n",
    "mode_name = 'Basic-LSTM(US)'\n",
    "US_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = basic_lstm.generate_data(CDC_norm, available_weeks,\n",
    "                                                  look_back, 1, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/basic_lstm_{epoch}.h5')\n",
    "\n",
    "history = basic_lstm.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 10\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/US_basic_lstm_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/basic_lstm_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "test_basic_lstm.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_basic_lstm.model.predict(left_shift)[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1.0 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "US_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFSI\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "# US_perform_by_model = {}\n",
    "mode_name = 'DEFSI(US)'\n",
    "US_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = defsi.generate_data(CDC_norm, available_weeks,\n",
    "                                              look_back, 1, \n",
    "                                              test_split_size=test_split_size,\n",
    "                                              val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/defsi_{epoch}.h5')\n",
    "\n",
    "history = defsi.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/US_defsi_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/defsi_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "test_defsi.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_defsi.model.predict([left_shift, np.array([x_total[1][target_idx+1]])])[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "US_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seq2Seq\n",
    "\n",
    "# US_perform_by_model = {}\n",
    "mode_name = 'Seq2Seq(US)'\n",
    "US_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "\n",
    "available_weeks = US_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = seq2seq.generate_data(CDC_norm, available_weeks,\n",
    "                                                  look_back, look_ahead, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/seq2seq_{epoch}.h5')\n",
    "\n",
    "history = seq2seq.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "\n",
    "best_path = './models/v2/US_seq2seq_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/seq2seq_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "test_seq2seq.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (CDC_norm.loc[start_week:52, '2018'].tolist() + CDC_norm.loc[:52, '2019'].tolist()\n",
    "                + CDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "\n",
    "for target_idx in range(90-1):\n",
    "    enc_out, enc_h, enc_c = test_seq2seq.encoder_model.predict(np.array([x_total[0][target_idx]]))\n",
    "\n",
    "    dec_input = x_total[1][target_idx][0].reshape(-1, 1)\n",
    "\n",
    "    tmp = []\n",
    "    for ahead in range(look_ahead):\n",
    "        prd, dec_h, dec_c = test_seq2seq.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "        prd_val = prd[0, 0]\n",
    "        if prd[0, 0] > 0:\n",
    "            tmp.append(prd_val)\n",
    "        else:\n",
    "            tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "        dec_input = prd[0, 0]\n",
    "        enc_h, enc_c = dec_h, dec_c\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "US_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = CDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([CDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    US_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    US_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    US_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    US_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    US_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    US_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_perform = pd.concat([US_perform, pd.DataFrame(US_perform_by_model).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US_perform = pd.DataFrame(US_perform_by_model).T\n",
    "US_perform.to_csv('./data/v2/US_perform6.csv', encoding='utf8')\n",
    "US_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/US_predict4.pkl', 'wb') as f:\n",
    "    pickle.dump(US_prd_by_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AU_trends MEDIF 적용\n",
    "epochs = 100\n",
    "is_sg = False\n",
    "# sg_path = '_sg' if is_sg else ''\n",
    "sg_path = '_2y'\n",
    "\n",
    "AU_perform_by_model = {}\n",
    "AU_prd_by_model = {}\n",
    "\n",
    "for i in [1]:\n",
    "#     perform_by_num_words['AU'][i] = {}\n",
    "    mode = 'T'\n",
    "\n",
    "    mode_name = 'ME-' + mode + '(AU)'\n",
    "    mode_name = mode_name+'(2Y)' # if is_sg else mode_name\n",
    "    \n",
    "    AU_perform_by_model[mode_name] = {}\n",
    "    print('mode:', mode_name)\n",
    "    clear_session()\n",
    "    \n",
    "    target_max_lag = AU_trends_max_lag.T.sort_values(by='coef', ascending=False).head(i).T\n",
    "    trend_word_num = len(target_max_lag.columns)\n",
    "    trend_word_num = trend_word_num+1 if is_sg else trend_word_num\n",
    "\n",
    "    me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                          past_year_num=5,\n",
    "                          trend_word_num=trend_word_num,\n",
    "                          mode=mode,\n",
    "                          is_sg=is_sg)\n",
    "\n",
    "    available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "    x, y, val_size, test_size = me.generate_data(ASPREN_norm, available_weeks,\n",
    "                                                 look_back, look_ahead,\n",
    "                                                 test_split_size=test_split_size,\n",
    "                                                 val_split_size=val_split_size,\n",
    "                                                 trends_data=AU_trends_norm,\n",
    "                                                 trends_lag=target_max_lag)\n",
    "\n",
    "    x_train, x_val, x_test, x_total = x\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "    cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "    history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                      callbacks=[cp], verbose=0)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    min_epoch = 45\n",
    "    target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                    min(history.history['val_loss'][min_epoch:]))\n",
    "    best_path = './models/v2/AU_me_' + mode + '_lb' + str(look_back) + sg_path + '_best.h5'\n",
    "    print(target_epoch, best_path)\n",
    "    shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "    test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=5,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode,\n",
    "                              is_sg=is_sg)\n",
    "    test_me.load_model(best_path)\n",
    "\n",
    "    start_week = int(available_weeks[0].split('-')[1])\n",
    "    end_week = int(available_weeks[-1].split('-')[1])\n",
    "    target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                    + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "    prd_total = []\n",
    "\n",
    "    for target_idx in range(90-1):\n",
    "        if is_sg:\n",
    "            enc_inputs = [x_total[0][target_idx:target_idx+1]]\n",
    "            dec_input = x_total[1][target_idx, 0, :].reshape(1, 1, x_total[1].shape[2])\n",
    "        else:\n",
    "            enc_inputs = [x_total[0][target_idx:target_idx+1],\n",
    "                          x_total[1][target_idx:target_idx+1]]\n",
    "            dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "\n",
    "        enc_out, enc_h, enc_c = test_me.encoder_model.predict(enc_inputs)\n",
    "\n",
    "        tmp = []\n",
    "        for ahead in range(look_ahead):\n",
    "            prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "            prd_val = prd[0, 0, 0]\n",
    "            if prd_val > 0:\n",
    "                tmp.append([prd_val])\n",
    "            else:\n",
    "                tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "            if ahead < look_ahead - 1:\n",
    "              if is_sg:\n",
    "                dec_input = x_total[1][target_idx, ahead+1, :].reshape(1, 1, x_total[1].shape[2])\n",
    "              else:\n",
    "                dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "              dec_input[0, 0, -1] = prd_val\n",
    "            enc_h, enc_c = dec_h, dec_c\n",
    "        prd_total.append(tmp)\n",
    "    prd_total = np.array(prd_total)\n",
    "    AU_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "    show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "    prd_train = prd_total[:val_size]\n",
    "    prd_val = prd_total[val_size:test_size]\n",
    "    prd_test = prd_total[test_size:]\n",
    "\n",
    "    inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "    inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                           inverse_prd_total[:val_size], look_ahead)\n",
    "    rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                     inverse_prd_total[val_size:test_size], look_ahead)\n",
    "    rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                        inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "    for ahead in range(look_ahead):\n",
    "        ah = str(ahead+1)\n",
    "        AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "        AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "        AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "        AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "        AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "        AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "        AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "        AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "        AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "        AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "        AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "        AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    perform_by_num_words['AU'][i]['perform'][mode_name] = AU_perform_by_model[mode_name].copy()\n",
    "    perform_by_num_words['AU'][i]['prd'][mode_name] = prd_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([AU_perform, pd.DataFrame(AU_perform_by_model).T])[['total_%s_%s' % (ev, i) for i in [1,2,3] for ev in ['rmse', 'mape', 'corr']]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic_LSTM\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# AU_perform_by_model = {}\n",
    "mode_name = 'Basic-LSTM(AU)'\n",
    "AU_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = basic_lstm.generate_data(ASPREN_norm, available_weeks,\n",
    "                                                  look_back, 1, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/basic_lstm_{epoch}.h5')\n",
    "\n",
    "history = basic_lstm.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 10\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/AU_basic_lstm_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/basic_lstm_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "test_basic_lstm.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_basic_lstm.model.predict(left_shift)[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1.0 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "AU_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFSI\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "# AU_perform_by_model = {}\n",
    "mode_name = 'DEFSI(AU)'\n",
    "AU_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = defsi.generate_data(ASPREN_norm, available_weeks,\n",
    "                                              look_back, 1, \n",
    "                                              test_split_size=test_split_size,\n",
    "                                              val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/defsi_{epoch}.h5')\n",
    "\n",
    "history = defsi.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/AU_defsi_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/defsi_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "test_defsi.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_defsi.model.predict([left_shift, np.array([x_total[1][target_idx+1]])])[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "AU_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seq2Seq\n",
    "\n",
    "# AU_perform_by_model = {}\n",
    "mode_name = 'Seq2Seq(AU)'\n",
    "AU_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "\n",
    "available_weeks = AU_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = seq2seq.generate_data(ASPREN_norm, available_weeks,\n",
    "                                                  look_back, look_ahead, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/seq2seq_{epoch}.h5')\n",
    "\n",
    "history = seq2seq.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "\n",
    "best_path = './models/v2/AU_seq2seq_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/seq2seq_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "test_seq2seq.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (ASPREN_norm.loc[start_week:52, '2018'].tolist() + ASPREN_norm.loc[:52, '2019'].tolist()\n",
    "                + ASPREN_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "\n",
    "for target_idx in range(90-1):\n",
    "    enc_out, enc_h, enc_c = test_seq2seq.encoder_model.predict(np.array([x_total[0][target_idx]]))\n",
    "\n",
    "    dec_input = x_total[1][target_idx][0].reshape(-1, 1)\n",
    "\n",
    "    tmp = []\n",
    "    for ahead in range(look_ahead):\n",
    "        prd, dec_h, dec_c = test_seq2seq.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "        prd_val = prd[0, 0]\n",
    "        if prd[0, 0] > 0:\n",
    "            tmp.append(prd_val)\n",
    "        else:\n",
    "            tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "        dec_input = prd[0, 0]\n",
    "        enc_h, enc_c = dec_h, dec_c\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "AU_prd_by_model[mode_name] = prd_total\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = ASPREN_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([ASPREN_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    AU_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    AU_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    AU_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    AU_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    AU_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    AU_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AU_perform = pd.concat([AU_perform, pd.DataFrame(AU_perform_by_model).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AU_perform = pd.DataFrame(AU_perform_by_model).T\n",
    "AU_perform.to_csv('./data/v2/AU_perform6.csv', encoding='utf8')\n",
    "AU_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/AU_predict4.pkl', 'wb') as f:\n",
    "    pickle.dump(AU_prd_by_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , look_back param check\n",
    "\n",
    "KR_perform_by_lookback = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for look_back in range(3, 11):\n",
    "    print('look back:', look_back)\n",
    "    KR_perform_by_lookback[look_back] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_lookback[look_back]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_lookback[look_back]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_lookback.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_lookback, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "for look_back in range(3, 11):\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lookback[look_back]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_lookback[look_back]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_compare = {}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    lb_compare[ev] = []\n",
    "    for lb in range(3, 11):\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_lookback[lb]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        lb_compare[ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "\n",
    "lb_compare = pd.DataFrame(lb_compare, index=[_ for _ in range(3, 11)])\n",
    "lb_compare['rmse(improve)'] = lb_compare['rmse'].apply(lambda x: (lb_compare['rmse'].max() - x) / lb_compare['rmse'].max())\n",
    "lb_compare['mape(improve)'] = lb_compare['mape'].apply(lambda x: (lb_compare['mape'].max() - x) / lb_compare['mape'].max())\n",
    "lb_compare['corr(improve)'] = lb_compare['corr'].apply(lambda x: (x - lb_compare['corr'].min()) / lb_compare['corr'].min())\n",
    "lb_compare['improve(mean)'] = lb_compare[['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "lb_compare.to_csv('./data/v2/KR_perform_by_lookback.csv', encoding='utf8')\n",
    "lb_compare.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , layer_size param check\n",
    "\n",
    "KR_perform_by_layersize = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for layer_size in [32, 64, 128, 256, 512]:\n",
    "    print('layer_size:', layer_size)\n",
    "    KR_perform_by_layersize[layer_size] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              layer_size=layer_size,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  layer_size=layer_size,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_layersize[layer_size]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_layersize[layer_size]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_layersize.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_layersize, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "for layer_size in [32, 64, 128, 256, 512]:\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_layersize[layer_size]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_layersize[layer_size]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_compare = {}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    ls_compare[ev] = []\n",
    "    for ls in [32, 64, 128, 256, 512]:\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_layersize[ls]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        ls_compare[ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "\n",
    "ls_compare = pd.DataFrame(ls_compare, index=[32, 64, 128, 256, 512])\n",
    "ls_compare['rmse(improve)'] = ls_compare['rmse'].apply(lambda x: (ls_compare['rmse'].max() - x) / ls_compare['rmse'].max())\n",
    "ls_compare['mape(improve)'] = ls_compare['mape'].apply(lambda x: (ls_compare['mape'].max() - x) / ls_compare['mape'].max())\n",
    "ls_compare['corr(improve)'] = ls_compare['corr'].apply(lambda x: (x - ls_compare['corr'].min()) / ls_compare['corr'].min())\n",
    "ls_compare['improve(mean)'] = ls_compare[['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "ls_compare.to_csv('./data/v2/KR_perform_by_layersize.csv', encoding='utf8')\n",
    "ls_compare.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , max_lag coef threshold param check\n",
    "\n",
    "KR_perform_by_coef = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for threshold in [0.0, 0.1, 0.2, 0.3, 0.4]:\n",
    "    \n",
    "    KR_news_tmp_max_lag = {}\n",
    "    lag_max = KR_news_lag.max()\n",
    "    lag_max = lag_max[lag_max > threshold]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_news_lag[word].argmax()\n",
    "        val = KR_news_lag.loc[lag, word]\n",
    "        if lag >= 0:\n",
    "            KR_news_tmp_max_lag[word] = (lag, val)\n",
    "    KR_news_tmp_max_lag = pd.DataFrame(KR_news_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_sns_tmp_max_lag = {}\n",
    "    lag_max = KR_sns_lag.max()\n",
    "    lag_max = lag_max[lag_max > threshold]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_sns_lag[word].argmax()\n",
    "        val = KR_sns_lag.loc[lag, word]\n",
    "        if lag >= 0:\n",
    "            KR_sns_tmp_max_lag[word] = (lag, val)\n",
    "    KR_sns_tmp_max_lag = pd.DataFrame(KR_sns_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_trends_tmp_max_lag = {}\n",
    "    lag_max = KR_trends_lag.max()\n",
    "    lag_max = lag_max[lag_max > threshold]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_trends_lag[word].argmax()\n",
    "        val = KR_trends_lag.loc[lag, word]\n",
    "        if lag >= 0:\n",
    "            KR_trends_tmp_max_lag[word] = (lag, val)\n",
    "    KR_trends_tmp_max_lag = pd.DataFrame(KR_trends_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    print('threshold:', threshold)\n",
    "    KR_perform_by_coef[threshold] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_tmp_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_tmp_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_tmp_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_tmp_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_tmp_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_tmp_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_coef[threshold]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_coef[threshold]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_coef.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_coef, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "for threshold in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_coef[threshold]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_coef[threshold]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_compare = {'N': {}, 'S': {}, 'T': {}, 'mean': {}}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    coef_compare['N'][ev] = []\n",
    "    coef_compare['S'][ev] = []\n",
    "    coef_compare['T'][ev] = []\n",
    "    coef_compare['mean'][ev] = []\n",
    "    for coef_threshold in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_coef[coef_threshold]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        n, s, t = tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).values\n",
    "        \n",
    "        coef_compare['N'][ev].append(n)\n",
    "        coef_compare['S'][ev].append(s)\n",
    "        coef_compare['T'][ev].append(t)\n",
    "        coef_compare['mean'][ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "        \n",
    "for mode in ['N', 'S', 'T', 'mean']:\n",
    "    coef_compare[mode] = pd.DataFrame(coef_compare[mode], index=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "    coef_compare[mode]['rmse(improve)'] = coef_compare[mode]['rmse'].apply(lambda x: (coef_compare[mode]['rmse'].max() - x) / coef_compare[mode]['rmse'].max())\n",
    "    coef_compare[mode]['mape(improve)'] = coef_compare[mode]['mape'].apply(lambda x: (coef_compare[mode]['mape'].max() - x) / coef_compare[mode]['mape'].max())\n",
    "    coef_compare[mode]['corr(improve)'] = coef_compare[mode]['corr'].apply(lambda x: (x - coef_compare[mode]['corr'].min()) / coef_compare[mode]['corr'].min())\n",
    "    coef_compare[mode]['improve(mean)'] = coef_compare[mode][['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "    coef_compare[mode].to_csv('./data/v2/KR_perform_by_coef_' + mode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_compare['mean'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF , max_lag lag threshold param check\n",
    "\n",
    "KR_perform_by_lag = {}\n",
    "KR_perform_by_model = {}\n",
    "KR_prd_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "mode = 'T'\n",
    "\n",
    "for threshold in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    \n",
    "    KR_news_tmp_max_lag = {}\n",
    "    lag_max = KR_news_lag.max()\n",
    "    lag_max = lag_max[lag_max > 0.5]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_news_lag[word].argmax()\n",
    "        val = KR_news_lag.loc[lag, word]\n",
    "        if lag >= threshold:\n",
    "            KR_news_tmp_max_lag[word] = (lag, val)\n",
    "    KR_news_tmp_max_lag = pd.DataFrame(KR_news_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_sns_tmp_max_lag = {}\n",
    "    lag_max = KR_sns_lag.max()\n",
    "    lag_max = lag_max[lag_max > 0.5]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_sns_lag[word].argmax()\n",
    "        val = KR_sns_lag.loc[lag, word]\n",
    "        if lag >= threshold:\n",
    "            KR_sns_tmp_max_lag[word] = (lag, val)\n",
    "    KR_sns_tmp_max_lag = pd.DataFrame(KR_sns_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    KR_trends_tmp_max_lag = {}\n",
    "    lag_max = KR_trends_lag.max()\n",
    "    lag_max = lag_max[lag_max > 0.5]\n",
    "    for word in lag_max.index:\n",
    "        lag = KR_trends_lag[word].argmax()\n",
    "        val = KR_trends_lag.loc[lag, word]\n",
    "        if lag >= threshold:\n",
    "            KR_trends_tmp_max_lag[word] = (lag, val)\n",
    "    KR_trends_tmp_max_lag = pd.DataFrame(KR_trends_tmp_max_lag, index=['max_lag', 'coef'])\n",
    "    \n",
    "    print('='*5, 'threshold:', threshold, '='*5)\n",
    "    KR_perform_by_lag[threshold] = {}\n",
    "    for mode in ['N', 'S', 'T', 'A']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        print('mode:', mode_name)\n",
    "\n",
    "        clear_session()\n",
    "\n",
    "        news_word_num = len(KR_news_tmp_max_lag.columns)\n",
    "        sns_word_num = len(KR_sns_tmp_max_lag.columns)\n",
    "        trend_word_num = len(KR_trends_tmp_max_lag.columns)\n",
    "\n",
    "        me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=6,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode)\n",
    "\n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                     look_back, look_ahead,\n",
    "                                                     test_split_size=test_split_size,\n",
    "                                                     val_split_size=val_split_size,\n",
    "                                                     news_data=KR_news_norm,\n",
    "                                                     news_lag=KR_news_tmp_max_lag,\n",
    "                                                     sns_data=KR_sns_norm,\n",
    "                                                     sns_lag=KR_sns_tmp_max_lag,\n",
    "                                                     trends_data=KR_trends_norm,\n",
    "                                                     trends_lag=KR_trends_tmp_max_lag)\n",
    "\n",
    "        x_train, x_val, x_test, x_total = x\n",
    "        y_train, y_val, y_test = y\n",
    "\n",
    "        print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "        cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "        history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                          callbacks=[cp], verbose=0)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train_loss')\n",
    "        plt.plot(history.history['val_loss'], label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        min_epoch = 100\n",
    "        target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                        min(history.history['val_loss'][min_epoch:]))\n",
    "        best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + '_best.h5'\n",
    "        print(target_epoch, best_path)\n",
    "        shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "        test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                                  past_year_num=past_year_num,\n",
    "                                  news_word_num=news_word_num,\n",
    "                                  sns_word_num=sns_word_num,\n",
    "                                  trend_word_num=trend_word_num,\n",
    "                                  mode=mode)\n",
    "        test_me.load_model(best_path)\n",
    "\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        prd_total = []\n",
    "\n",
    "        for target_idx in range(90-1-look_back+8):\n",
    "            if mode == 'A':\n",
    "                enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                            [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                             x_total[2][target_idx:target_idx+1],\n",
    "                                                                                             x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "                dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                      + [news_states, sns_states, trend_states])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                             x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                             x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                        dec_input[0][0, 0, -1] = prd_val\n",
    "                        dec_input[1][0, 0, -1] = prd_val\n",
    "                        dec_input[2][0, 0, -1] = prd_val\n",
    "            else:\n",
    "                enc_out, enc_h, enc_c = test_me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                      x_total[1][target_idx:target_idx+1]])\n",
    "\n",
    "                dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                tmp = []\n",
    "                for ahead in range(look_ahead):\n",
    "                    prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                    prd_val = prd[0, 0, 0]\n",
    "                    if prd_val > 0:\n",
    "                        tmp.append([prd_val])\n",
    "                    else:\n",
    "                        tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                    if ahead < look_ahead - 1:\n",
    "                        dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                        dec_input[0, 0, -1] = prd_val\n",
    "                    enc_h, enc_c = dec_h, dec_c\n",
    "            prd_total.append(tmp)\n",
    "        prd_total = np.array(prd_total)\n",
    "        KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "        show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "        prd_train = prd_total[:val_size]\n",
    "        prd_val = prd_total[val_size:test_size]\n",
    "        prd_test = prd_total[test_size:]\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(target_truth, \n",
    "                                                               prd_total, look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(target_truth[:val_size+look_ahead], \n",
    "                                                               prd_train, look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(target_truth[val_size:test_size+look_ahead],\n",
    "                                                         prd_val, look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(target_truth[test_size:],\n",
    "                                                            prd_test, look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "#     print(KR_perform_by_model)\n",
    "    KR_perform_by_lag[threshold]['perform'] = KR_perform_by_model.copy()\n",
    "    KR_perform_by_lag[threshold]['predict'] = KR_prd_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_perform_by_lag.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_perform_by_lag, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform_by_model = {}\n",
    "\n",
    "look_back = 7\n",
    "\n",
    "for threshold in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for mode in ['N', 'S', 'T']:\n",
    "        mode_name = 'ME-' + mode\n",
    "        KR_perform_by_model[mode_name] = {}\n",
    "        \n",
    "        available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "        start_week = int(available_weeks[0].split('-')[1])\n",
    "        end_week = int(available_weeks[-1].split('-')[1])\n",
    "        \n",
    "        target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                        + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "        rmse_total, mape_total, corr_total = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1)), \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]]),\n",
    "                                                               look_ahead)\n",
    "        rmse_train, mape_train, corr_train = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[:val_size+look_ahead], \n",
    "                                                               np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]])[:val_size],\n",
    "                                                               look_ahead)\n",
    "        rmse_val, mape_val, corr_val = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[val_size:test_size+look_ahead],\n",
    "                                                         np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]])[val_size:test_size],\n",
    "                                                         look_ahead)\n",
    "        rmse_test, mape_test, corr_test = Evaluate.evaluate(KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))[test_size:],\n",
    "                                                            np.array([KCDC_scaler.inverse_transform(_) for _ in KR_perform_by_lag[threshold]['predict'][mode_name]])[test_size:],\n",
    "                                                            look_ahead)\n",
    "\n",
    "        for ahead in range(look_ahead):\n",
    "            ah = str(ahead+1)\n",
    "            KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "            KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "            KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "            KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "            KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "            KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    KR_perform_by_lag[threshold]['perform'] = KR_perform_by_model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_compare = {'N': {}, 'S': {}, 'T': {}, 'mean': {}}\n",
    "\n",
    "for ev in ['rmse', 'mape', 'corr']:\n",
    "    lag_compare['N'][ev] = []\n",
    "    lag_compare['S'][ev] = []\n",
    "    lag_compare['T'][ev] = []\n",
    "    lag_compare['mean'][ev] = []\n",
    "    for lag_threshold in np.arange(0, 11):\n",
    "        tmp2 = pd.DataFrame(KR_perform_by_lag[lag_threshold]['perform']).T.loc[['ME-N', 'ME-S', 'ME-T']]\n",
    "        n, s, t = tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).values\n",
    "        \n",
    "        lag_compare['N'][ev].append(n)\n",
    "        lag_compare['S'][ev].append(s)\n",
    "        lag_compare['T'][ev].append(t)\n",
    "        lag_compare['mean'][ev].append(tmp2[[c for c in tmp2.columns if 'total' in c and ev in c]].mean(axis=1).mean())\n",
    "        \n",
    "for mode in ['N', 'S', 'T', 'mean']:\n",
    "    lag_compare[mode] = pd.DataFrame(lag_compare[mode], index=np.arange(0, 11))\n",
    "    lag_compare[mode]['rmse(improve)'] = lag_compare[mode]['rmse'].apply(lambda x: (lag_compare[mode]['rmse'].max() - x) / lag_compare[mode]['rmse'].max())\n",
    "    lag_compare[mode]['mape(improve)'] = lag_compare[mode]['mape'].apply(lambda x: (lag_compare[mode]['mape'].max() - x) / lag_compare[mode]['mape'].max())\n",
    "    lag_compare[mode]['corr(improve)'] = lag_compare[mode]['corr'].apply(lambda x: (x - lag_compare[mode]['corr'].min()) / lag_compare[mode]['corr'].min())\n",
    "    lag_compare[mode]['improve(mean)'] = lag_compare[mode][['rmse(improve)', 'mape(improve)', 'corr(improve)']].mean(axis=1)\n",
    "    lag_compare[mode].to_csv('./data/v2/KR_perform_by_lag_' + mode + '.csv', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_compare['mean'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KR_N,S,T MEDIF 적용\n",
    "epochs = 200\n",
    "is_sg = False\n",
    "sg_path = '_2y' # if is_sg else ''\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "# KR_prd_by_model = {}\n",
    "\n",
    "mode_num_words = {'N': 10, 'S': 6, 'T': 9}\n",
    "#     perform_by_num_words['KR'][i] = {}\n",
    "for mode in ['S']:\n",
    "    i = mode_num_words[mode]\n",
    "    mode_name = 'ME-' + mode\n",
    "    mode_name = mode_name+'(2Y)' # if is_sg else mode_name\n",
    "    \n",
    "    KR_perform_by_model[mode_name] = {}\n",
    "    print('mode:', mode_name)\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    news_target_max_lag = KR_news_max_lag.T.sort_values(by='coef', ascending=False).T\n",
    "    sns_target_max_lag = KR_sns_max_lag.T.sort_values(by='coef', ascending=False).head(6).T\n",
    "    trends_target_max_lag = KR_trends_max_lag.T.sort_values(by='coef', ascending=False).head(9).T\n",
    "\n",
    "    if mode == 'N':\n",
    "        print(i, 'news', news_target_max_lag.columns)\n",
    "    elif mode == 'S':\n",
    "        print(i, 'sns', sns_target_max_lag.columns)\n",
    "    else:\n",
    "        print(i, 'trends', trends_target_max_lag.columns)\n",
    "\n",
    "    news_word_num = len(news_target_max_lag.columns)\n",
    "    news_word_num = news_word_num+1 if is_sg else news_word_num\n",
    "    sns_word_num = len(sns_target_max_lag.columns)\n",
    "    sns_word_num = sns_word_num+1 if is_sg else sns_word_num\n",
    "    trend_word_num = len(trends_target_max_lag.columns)\n",
    "    trend_word_num = trend_word_num+1 if is_sg else trend_word_num\n",
    "\n",
    "    me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                          past_year_num=4,\n",
    "                          news_word_num=news_word_num,\n",
    "                          sns_word_num=sns_word_num,\n",
    "                          trend_word_num=trend_word_num,\n",
    "                          mode=mode,\n",
    "                          is_sg=is_sg)\n",
    "\n",
    "    available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "    x, y, val_size, test_size = me.generate_data(KCDC_norm, available_weeks,\n",
    "                                                 look_back, look_ahead,\n",
    "                                                 test_split_size=test_split_size,\n",
    "                                                 val_split_size=val_split_size,\n",
    "                                                 news_data=KR_news_norm,\n",
    "                                                 news_lag=news_target_max_lag,\n",
    "                                                 sns_data=KR_sns_norm,\n",
    "                                                 sns_lag=sns_target_max_lag,\n",
    "                                                 trends_data=KR_trends_norm,\n",
    "                                                 trends_lag=trends_target_max_lag)\n",
    "\n",
    "    x_train, x_val, x_test, x_total = x\n",
    "    y_train, y_val, y_test = y\n",
    "\n",
    "    print('val size:', val_size, 'test_size:', test_size)\n",
    "\n",
    "    cp = ModelCheckpoint('./models/v2/me_{epoch}.h5')\n",
    "\n",
    "    history = me.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                      callbacks=[cp], verbose=0)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    min_epoch = 100\n",
    "    target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                    min(history.history['val_loss'][min_epoch:]))\n",
    "    best_path = './models/v2/KR_me_' + mode + '_lb' + str(look_back) + sg_path + '_best.h5'\n",
    "    print(target_epoch, best_path)\n",
    "    shutil.copy('./models/v2/me_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "\n",
    "    test_me = Multi_Encoder_v2(look_back=look_back, look_ahead=look_ahead,\n",
    "                              past_year_num=4,\n",
    "                              news_word_num=news_word_num,\n",
    "                              sns_word_num=sns_word_num,\n",
    "                              trend_word_num=trend_word_num,\n",
    "                              mode=mode,\n",
    "                              is_sg=is_sg)\n",
    "    test_me.load_model(best_path)\n",
    "\n",
    "    start_week = int(available_weeks[0].split('-')[1])\n",
    "    end_week = int(available_weeks[-1].split('-')[1])\n",
    "    target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                    + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "    prd_total = []\n",
    "\n",
    "    for target_idx in range(90-1):\n",
    "        if mode == 'A':\n",
    "            enc_outs, news_states, sns_states, trend_states = me.encoder_model.predict([x_total[0][target_idx:target_idx+1],\n",
    "                                                                                        [x_total[1][target_idx:target_idx+1],\n",
    "                                                                                         x_total[2][target_idx:target_idx+1],\n",
    "                                                                                         x_total[3][target_idx:target_idx+1]]])\n",
    "\n",
    "            dec_input = [x_total[4][target_idx, 0, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                         x_total[5][target_idx, 0, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                         x_total[6][target_idx, 0, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "            tmp = []\n",
    "            for ahead in range(look_ahead):\n",
    "                prd, news_states, sns_states, trend_states = me.decoder_model.predict(dec_input + enc_outs \n",
    "                                                                  + [news_states, sns_states, trend_states])\n",
    "                prd_val = prd[0, 0, 0]\n",
    "                if prd_val > 0:\n",
    "                    tmp.append([prd_val])\n",
    "                else:\n",
    "                    tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                if ahead < look_ahead - 1:\n",
    "                    dec_input = [x_total[4][target_idx, ahead+1, :].reshape(1, 1, x_total[4].shape[2]),\n",
    "                         x_total[5][target_idx, ahead+1, :].reshape(1, 1, x_total[5].shape[2]),\n",
    "                         x_total[6][target_idx, ahead+1, :].reshape(1, 1, x_total[6].shape[2])]\n",
    "                    dec_input[0][0, 0, -1] = prd_val\n",
    "                    dec_input[1][0, 0, -1] = prd_val\n",
    "                    dec_input[2][0, 0, -1] = prd_val\n",
    "        else:\n",
    "            if is_sg:\n",
    "              enc_inputs = [x_total[0][target_idx:target_idx+1]]\n",
    "              dec_input = x_total[1][target_idx, 0, :].reshape(1, 1, x_total[1].shape[2])\n",
    "            else:\n",
    "              enc_inputs = [x_total[0][target_idx:target_idx+1],\n",
    "                            x_total[1][target_idx:target_idx+1]]\n",
    "              dec_input = x_total[2][target_idx, 0, :].reshape(1, 1, x_total[2].shape[2])\n",
    "            enc_out, enc_h, enc_c = test_me.encoder_model.predict(enc_inputs)\n",
    "\n",
    "            tmp = []\n",
    "            for ahead in range(look_ahead):\n",
    "                prd, dec_h, dec_c = test_me.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "                prd_val = prd[0, 0, 0]\n",
    "                if prd_val > 0:\n",
    "                    tmp.append([prd_val])\n",
    "                else:\n",
    "                    tmp.append([np.random.uniform(1e-6, 1e-5)])\n",
    "\n",
    "                if ahead < look_ahead - 1:\n",
    "                    if is_sg:\n",
    "                      dec_input = x_total[1][target_idx, ahead+1, :].reshape(1, 1, x_total[1].shape[2])\n",
    "                    else:\n",
    "                      dec_input = x_total[2][target_idx, ahead+1, :].reshape(1, 1, x_total[2].shape[2])\n",
    "                    dec_input[0, 0, -1] = prd_val\n",
    "                enc_h, enc_c = dec_h, dec_c\n",
    "        prd_total.append(tmp)\n",
    "    prd_total = np.array(prd_total)\n",
    "    KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "    show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "    prd_train = prd_total[:val_size]\n",
    "    prd_val = prd_total[val_size:test_size]\n",
    "    prd_test = prd_total[test_size:]\n",
    "\n",
    "    inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "    inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                           inverse_prd_total, look_ahead)\n",
    "    rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                           inverse_prd_total[:val_size], look_ahead)\n",
    "    rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                     inverse_prd_total[val_size:test_size], look_ahead)\n",
    "    rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                        inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "    for ahead in range(look_ahead):\n",
    "        ah = str(ahead+1)\n",
    "        KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "        KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "        KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "        KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "        KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "        KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "        KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "        KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "        KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "        KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "        KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "        KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]\n",
    "    perform_by_num_words['KR'][i]['perform'][mode_name] = KR_perform_by_model[mode_name].copy()\n",
    "    perform_by_num_words['KR'][i]['prd'][mode_name] = prd_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([KR_perform, pd.DataFrame(KR_perform_by_model).T])[\n",
    "  ['total_%s_%s' % (ev, i) for i in [5, 8, 10] for ev in ['rmse', 'mape', 'corr']]].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "ev = 'rmse'\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['US'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(US)'].values\n",
    "axs[0].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['US'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(US)(SG)'].values\n",
    "axs[0].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['AU'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(AU)'].values\n",
    "axs[1].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['AU'][1]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(AU)(SG)'].values\n",
    "axs[1].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][10]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-N'].values\n",
    "axs[2].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][10]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-N(SG)'].values\n",
    "axs[2].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][6]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-S'].values\n",
    "axs[2].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][6]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-S(SG)'].values\n",
    "axs[2].plot(perform_vals, ls='--')\n",
    "\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T'].values\n",
    "axs[2].plot(perform_vals)\n",
    "perform_vals = pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "  ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "].T['ME-T(SG)'].values\n",
    "axs[2].plot(perform_vals, ls='--')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개 국가의 1, 5, 10주 예측 그래프\n",
    "\n",
    "markers = ['.', 's', 'D', 'v', 'h']\n",
    "colors = ['tab:brown', 'tab:purple', 'tab:pink', 'tab:olive', 'tab:blue']\n",
    "line_styles = ['--', '-.', '--', '-', '-']\n",
    "x_labels = ['week\\n(a) ahead-', 'week\\n(b) ahead-', 'week\\n(c) ahead-']\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "for col, ev in enumerate(['rmse', 'mape', 'corr']):\n",
    "    target_df = US_perform[[c for c in US_perform if 'total' in c and ev in c]]\n",
    "    for i, m in enumerate(['Basic-LSTM(US)', 'DEFSI(US)', 'Seq2Seq(US)', 'ME-T(US)(SG)', 'ME-T(US)']):\n",
    "        perform_vals = (target_df.T[m].values if 'ME-' not in m \n",
    "                        else pd.DataFrame(perform_by_num_words['US'][1]['perform']).T[\n",
    "                            ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                        ].T[m].values.reshape(-1))\n",
    "        axs[0, col].plot(perform_vals, label='MEDIF-' + m[3:], marker=markers[i], c=colors[i],\n",
    "                ls=line_styles[i])\n",
    "        axs[0, col].set_ylabel(ev.upper())\n",
    "        axs[0, col].set_xlabel('Prediction-period (weeks)')\n",
    "        axs[0, col].set_xticks(range(1, 11, 2))\n",
    "        axs[0, col].set_xticklabels(range(2, 12, 2))\n",
    "        axs[0, col].grid(True, ls=':')\n",
    "axs[0, 1].set_xlabel('Prediction-period (weeks)\\n(a) United States')\n",
    "        \n",
    "for col, ev in enumerate(['rmse', 'mape', 'corr']):\n",
    "    target_df = AU_perform[[c for c in AU_perform if 'total' in c and ev in c]]\n",
    "    for i, m in enumerate(['Basic-LSTM(AU)', 'DEFSI(AU)', 'Seq2Seq(AU)', 'ME-T(AU)(SG)', 'ME-T(AU)']):\n",
    "        perform_vals = (target_df.T[m].values if 'ME-' not in m \n",
    "                        else pd.DataFrame(perform_by_num_words['AU'][1]['perform']).T[\n",
    "                            ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                        ].T[m].values.reshape(-1))\n",
    "        \n",
    "        axs[1, col].plot(perform_vals, label='MEDIF-' + m[3:], marker=markers[i], c=colors[i],\n",
    "                ls=line_styles[i])\n",
    "        axs[1, col].set_ylabel(ev.upper())\n",
    "        axs[1, col].set_xlabel('Prediction-period (weeks)')\n",
    "        axs[1, col].set_xticks(range(1, 11, 2))\n",
    "        axs[1, col].set_xticklabels(range(2, 12, 2))\n",
    "        axs[1, col].grid(True, ls=':')\n",
    "axs[1, 1].set_xlabel('Prediction-period (weeks)\\n(b) Australia')\n",
    "        \n",
    "markers = ['.', 'v', 's', 'p', 'D', 'h', '*']\n",
    "colors = ['tab:brown', 'tab:purple', 'tab:pink', 'tab:red', 'tab:green', 'tab:blue']\n",
    "line_styles = ['--', '-.', '--', '-', '-', '-']\n",
    "for col, ev in enumerate(['rmse', 'mape', 'corr']):\n",
    "    target_df = KR_perform[[c for c in KR_perform if 'total' in c and ev in c]]\n",
    "    for i, m in enumerate(['Basic-LSTM', 'DEFSI', 'Seq2Seq', 'ME-N', 'ME-S', 'ME-T']):\n",
    "        if 'ME-' not in m:\n",
    "            perform_vals = target_df.T[m].values\n",
    "        else:\n",
    "            if m.split('-')[1] == 'N':\n",
    "                perform_vals = target_df.T[m].values\n",
    "            elif m.split('-')[1] == 'S':\n",
    "                perform_vals = pd.DataFrame(perform_by_num_words['KR'][6]['perform']).T[\n",
    "                                ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                                ].T[m].values\n",
    "            else:\n",
    "                perform_vals = pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "                                ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "                                ].T[m].values\n",
    "                \n",
    "                \n",
    "#         perform_vals = (target_df.T[m].values if 'ME-' not in m \n",
    "#                         else pd.DataFrame(perform_by_num_words['KR'][9]['perform']).T[\n",
    "#                             ['total_%s_%s' % (ev, i) for i in range(1, 11)]\n",
    "#                         ].T[m].values)\n",
    "        \n",
    "        axs[2, col].plot(perform_vals, marker=markers[i], c=colors[i],\n",
    "                ls=line_styles[i], label=m)\n",
    "        axs[2, col].set_ylabel(ev.upper())\n",
    "        axs[2, col].set_xlabel('Prediction-period (weeks)')\n",
    "        axs[2, col].set_xticks(range(1, 11, 2))\n",
    "        axs[2, col].set_xticklabels(range(2, 12, 2))\n",
    "        axs[2, col].grid(True, ls=':')\n",
    "axs[2, 1].set_xlabel('Prediction-period (weeks)\\n(c) Korea')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(-0.7, -0.25), ncol=6,\n",
    "          labels=['LSTM', 'DEFSI', 'STS-ATT',\n",
    "                  'Proposed(N)', 'Proposed(S)', 'Proposed(Q)'])\n",
    "# plt.savefig('./image/v3/All_perform_apply_num_words_down_dpi.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/perform_by_num_words2.pkl', 'wb') as f:\n",
    "    pickle.dump(perform_by_num_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic_LSTM\n",
    "\n",
    "epochs = 70\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "mode_name = 'Basic-LSTM'\n",
    "KR_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = basic_lstm.generate_data(KCDC_norm, available_weeks,\n",
    "                                                  look_back, 1, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/basic_lstm_{epoch}.h5')\n",
    "\n",
    "history = basic_lstm.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 30\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/KR_basic_lstm_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/basic_lstm_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_basic_lstm = Basic_LSTM(look_back=look_back, look_ahead=look_ahead)\n",
    "test_basic_lstm.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_basic_lstm.model.predict(left_shift)[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        prd = 1.0 - np.random.uniform(1e-6, 1e-5) if prd > 1 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFSI\n",
    "\n",
    "test_split_size = 0.65\n",
    "val_split_size = 0.7\n",
    "epochs = 300\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "mode_name = 'DEFSI'\n",
    "KR_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = defsi.generate_data(KCDC_norm, available_weeks,\n",
    "                                              look_back, 1, \n",
    "                                              test_split_size=test_split_size,\n",
    "                                              val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/defsi_{epoch}.h5')\n",
    "\n",
    "history = defsi.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 100\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "best_path = './models/v2/KR_defsi_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/defsi_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_defsi = DEFSI(look_back=look_back, look_ahead=1)\n",
    "test_defsi.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "for target_idx in range(90-1):    \n",
    "    tmp = []\n",
    "    for i in range(look_ahead):\n",
    "        if i == 0:\n",
    "            left_shift = np.array([x_total[0][target_idx]])\n",
    "        else:\n",
    "            left_shift = np.roll(left_shift, -1)\n",
    "            left_shift[:, -1, :] = tmp[-1]\n",
    "        prd = test_defsi.model.predict([left_shift, np.array([x_total[1][target_idx+1]])])[0, 0]\n",
    "        prd = np.random.uniform(1e-6, 1e-5) if prd < 0 else prd\n",
    "        tmp.append(prd)\n",
    "#     print(left_shift)\n",
    "#     print(tmp)\n",
    "    prd_total.append(tmp)\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "prd_total = np.array(prd_total)\n",
    "prd_total = prd_total.reshape((prd_total.shape[0], prd_total.shape[1], 1))\n",
    "KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seq2Seq\n",
    "\n",
    "# KR_perform_by_model = {}\n",
    "mode_name = 'Seq2Seq'\n",
    "KR_perform_by_model[mode_name] = {}\n",
    "print(mode_name)\n",
    "clear_session()\n",
    "\n",
    "seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "\n",
    "available_weeks = KR_trends_norm['weeks'].values[collect_err+max_lag+look_back:-look_ahead+1]\n",
    "\n",
    "x, y, val_size, test_size = seq2seq.generate_data(KCDC_norm, available_weeks,\n",
    "                                                  look_back, look_ahead, \n",
    "                                                  test_split_size=test_split_size,\n",
    "                                                  val_split_size=val_split_size)\n",
    "x_train, x_val, x_test, x_total = x\n",
    "y_train, y_val, y_test = y\n",
    "\n",
    "cp = ModelCheckpoint('./models/v2/seq2seq_{epoch}.h5')\n",
    "\n",
    "history = seq2seq.fit(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size,\n",
    "                  callbacks=[cp], verbose=0)\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "min_epoch = 200\n",
    "target_epoch = min_epoch + history.history['val_loss'][min_epoch:].index(\n",
    "                                min(history.history['val_loss'][min_epoch:]))\n",
    "\n",
    "best_path = './models/v2/KR_seq2seq_best.h5'\n",
    "print(target_epoch, best_path)\n",
    "shutil.copy('./models/v2/seq2seq_' + str(target_epoch) + '.h5', best_path)\n",
    "\n",
    "test_seq2seq = Seq2Seq(look_back=look_back, look_ahead=look_ahead)\n",
    "test_seq2seq.load_model(best_path)\n",
    "\n",
    "start_week = int(available_weeks[0].split('-')[1])\n",
    "end_week = int(available_weeks[-1].split('-')[1])\n",
    "target_truth = (KCDC_norm.loc[start_week:52, '2018'].tolist() + KCDC_norm.loc[:52, '2019'].tolist()\n",
    "                + KCDC_norm.loc[:end_week+look_ahead-1, '2020'].tolist())\n",
    "\n",
    "prd_total = []\n",
    "\n",
    "for target_idx in range(90-1):\n",
    "    enc_out, enc_h, enc_c = test_seq2seq.encoder_model.predict(np.array([x_total[0][target_idx]]))\n",
    "\n",
    "    dec_input = x_total[1][target_idx][0].reshape(-1, 1)\n",
    "\n",
    "    tmp = []\n",
    "    for ahead in range(look_ahead):\n",
    "        prd, dec_h, dec_c = test_seq2seq.decoder_model.predict([dec_input] + [enc_out, enc_h, enc_c])\n",
    "        prd_val = prd[0, 0]\n",
    "        if prd[0, 0] > 0:\n",
    "            tmp.append(prd_val)\n",
    "        else:\n",
    "            tmp.append([np.random.uniform(1e-4, 1e-3)])\n",
    "\n",
    "        dec_input = prd[0, 0]\n",
    "        enc_h, enc_c = dec_h, dec_c\n",
    "    prd_total.append(tmp)\n",
    "prd_total = np.array(prd_total)\n",
    "show_graph(target_truth, prd_total, look_ahead, val_size, test_size)\n",
    "prd_total = np.array(prd_total)\n",
    "KR_prd_by_model[mode_name] = prd_total\n",
    "\n",
    "prd_train = prd_total[:val_size]\n",
    "prd_val = prd_total[val_size:test_size]\n",
    "prd_test = prd_total[test_size:]\n",
    "\n",
    "inverse_target_truth = KCDC_scaler.inverse_transform(np.array(target_truth).reshape(-1, 1))\n",
    "inverse_prd_total = np.array([KCDC_scaler.inverse_transform(_) for _ in prd_total])\n",
    "\n",
    "rmse_total, mape_total, corr_total = Evaluate.evaluate(inverse_target_truth, \n",
    "                                                       inverse_prd_total, look_ahead)\n",
    "rmse_train, mape_train, corr_train = Evaluate.evaluate(inverse_target_truth[:val_size+look_ahead], \n",
    "                                                       inverse_prd_total[:val_size], look_ahead)\n",
    "rmse_val, mape_val, corr_val = Evaluate.evaluate(inverse_target_truth[val_size:test_size+look_ahead],\n",
    "                                                 inverse_prd_total[val_size:test_size], look_ahead)\n",
    "rmse_test, mape_test, corr_test = Evaluate.evaluate(inverse_target_truth[test_size:],\n",
    "                                                    inverse_prd_total[test_size:], look_ahead)\n",
    "\n",
    "for ahead in range(look_ahead):\n",
    "    ah = str(ahead+1)\n",
    "    KR_perform_by_model[mode_name]['total_rmse_' + ah] = rmse_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_mape_' + ah] = mape_total[ahead]\n",
    "    KR_perform_by_model[mode_name]['total_corr_' + ah] = corr_total[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['train_rmse_' + ah] = rmse_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_mape_' + ah] = mape_train[ahead]\n",
    "    KR_perform_by_model[mode_name]['train_corr_' + ah] = corr_train[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['val_rmse_' + ah] = rmse_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_mape_' + ah] = mape_val[ahead]\n",
    "    KR_perform_by_model[mode_name]['val_corr_' + ah] = corr_val[ahead]\n",
    "\n",
    "    KR_perform_by_model[mode_name]['test_rmse_' + ah] = rmse_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_mape_' + ah] = mape_test[ahead]\n",
    "    KR_perform_by_model[mode_name]['test_corr_' + ah] = corr_test[ahead]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KR_perform = pd.concat([KR_perform, pd.DataFrame(KR_perform_by_model).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KR_perform = pd.DataFrame(KR_perform_by_model).T\n",
    "KR_perform.to_csv('./data/v2/KR_perform6.csv', encoding='utf8')\n",
    "KR_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KR perform table\n",
    "\n",
    "KR_perform_mini = KR_perform[['total_rmse_1', 'total_mape_1', 'total_corr_1', \n",
    "            'total_rmse_5', 'total_mape_5', 'total_corr_5', \n",
    "            'total_rmse_10', 'total_mape_10', 'total_corr_10', ]].loc[['Basic-LSTM', 'DEFSI', 'Seq2Seq', 'ME-N', 'ME-S', 'ME-T']].round(3)\n",
    "KR_perform_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/v2/KR_predict4.pkl', 'wb') as f:\n",
    "    pickle.dump(KR_prd_by_model, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "py38tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
